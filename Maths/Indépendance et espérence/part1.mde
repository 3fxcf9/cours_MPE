# Indépendance et espérence de variables aléatoires

:date 09/02/2026

## Indépendance de variables aléatoires toutes définies sur le même espace probabilisé $(\Omega,\Tribu, \P)$

%def
  - Soient $X: (\Omega,\Tribu,\P) \longrightarrow E$ et $Y: (\Omega,\Tribu,\P) \longrightarrow E'$ deux variables aléatoires discrètes. On dit que $X$ et $Y$ sont indépendantes si
    $$
      \forall (A,B) \in \Part(X(\Omega)) \times \Part(Y(\Omega)) , \P \big((X \in A) \cap (Y \in B)\big) = \P(X \in A)\P(Y \in B)
    $$
    et on note $X \indep Y$.
  - $\big(X_{i}: (\Omega, \Tribu,\P) \longrightarrow E_{i}\big)_{i \in I}$ est une famille de variables aléatoires (mutuellement) indépendantes si pour toute famille $(B_{i})_{i \in I} \in \prod_{i \in I}\Part(X_{i}(\Omega))$, une des deux propositions équivalentes ci-dessous est vérifiée
    - pour toute partie finie non vide $J$ de $I$, on a
      $$
        \P \left(\bigcap_{i \in J} (X_{i} \in B_{i})\right) = \prod_{i \in J}\P(X_{i} \in B_{i})
      $$
    - $(X_{i} \in B_{i})_{i \in I}$ est une famille d’événements $\P$-mutuellement indépendants.
%

%eg
  1. Soit $A \in \Tribu$ avec $\P(A) \in \oo{0,1}$. A-t-on $\1_{A} \indep \1_{\bar{A}}$ ?
     ~
     $\P(\1_{A} \in \{1\}, \1_{\bar{A}} \in \{0\}) = \P(\1_{A} \in \{1\}) = \P(A)$ et $\P(\1_{A} \in \{1\}) \times  \P(\1_{\bar{A}} \in \{0\}) = \P(A)^{2}$ or $\P(A) \notin \{0,1\}$ donc $\P(A) \ne \P(A)^{2}$.
  2. Soit $(A_{i})_{i \in I}$ une famille d’événements. $(\1_{A_{i}})_{i \in I}$ est une famille de variables aléatoires indépendantes si et seulement si $(A_{i})_{i \in I}$ est une famille d’événements mutuellement indépendants.
     ~
     - Supposons $(A_{A_{i}})_{i \in I}$ une famille de variables aléatoires indépendantes. Soit $J$ une partie finie de $I$.
       $$
         \P \left(\bigcap_{i \in J} A_{i}\right) = \P \left(\bigcap_{i \in J} \big(\1_{A_{i}} \in \{1\}\big)\right) = \prod_{i \in J} \P(\1_{A_{i}} \in \{1\}) = \prod_{i \in J}\P(A_{i})
       $$
     - Le sens réciproque est laissé en exercice.
%

%prop
  1. Si $(X_{i})_{i \in I}$ est une famille de variables aléatoires discrètes indépendantes, alors pour toute partie $J$ de $I$ (non vide), $(X_{i})_{i \in J}$ est une famille de variables aléatoires indépendantes.
  2. Soit $(X_{n})_{n \in \N}$ une suite de variables aléatoires (toutes définies sur $(\Omega, \Tribu, \P)$). Les deux propriétés suivantes sont équivalentes
     - $(X_{n})_{n \in \N}$ est une suite de variables aléatoires indépendantes
     -  $\forall p \in \N$, $(X_{k})_{k \in \icc{0,p}}$ est une suite de variables aléatoires indépendantes.
     - $\displaystyle\forall p \in \N, \forall (B_{i})_{i \in \icc{0,p}} \in \prod_{i=0}^{p}\Part(X_{i}(\Omega)), \P \left(\bigcap_{i=0}^{p}(X_{i} \in B_{i})\right) = \prod_{i=0}^{p}\P(X_{i} \in B_{i})$((Ce qui est faux avec les événements.))
  3. Soient $X_{1}, \ldots, X_{n}$ des variables aléatoires discrètes définies sur $(\Omega,\Tribu,\P)$ à valeurs respectivement dans $E_{1}, \ldots, E_{n}$. Alors, $X_{1}, \ldots, X_{n}$ sont indépendantes si et seulement si
     $$
       \forall (x_{1}, \ldots, x_{n}) \in X_{1}(\Omega) \times  \cdots \times X_{n}(\Omega), \P(X_{1} = x_{1}, \ldots, X_{n} = x_{n}) = \prod_{i=1}^{n}\P(X_{i}=x_{i})
     $$
%

%proof
  1. Par définition.
  2. Le premier point donne le sens direct. Supposons $(X_{k})_{k \in \icc{0,p}}$ indépendante pour tout $p \in \N$. Soit $J$ une partie finie de $\N$. Soit $(B_{i})_{i \in J} \in \prod_{i \in J}X_{i}(\Omega)$. On a $J \subset \icc{0,\max J} =I$. Posons, pour tout $i \in I$,
     ~ $B_{i}' = B_{i}$ si $i \in J$  ~ $B_{i}' = X_{i}(\Omega)$ si $i \in I \setminus J$
     On a alors
     $$
       \P \left(\bigcap_{i \in J} (X_{i} \in B_{i})\right) \oeq{(1)} \P \left(\bigcap_{i \in I}(X_{i} \in B_{i}')\right) = \prod_{i \in I} \P(X_{i} \in B_{i}') \oeq{(2)} \prod_{i \in J} \P(X_{i} \in B_{i})
     $$
     (1) car $(X_{i} \in X_{i}(\Omega)) = \Omega$ et (2) car $\P(X_{i} \in X_{1}(\Omega))=1$.
  3. - Supposons $X_{1}, \ldots, X_{n}$ indépendantes. Alors pour tout $(x_{1}, \ldots, x_{n}) \in X_{1}(\Omega) \times \cdots \times X_{n}(\Omega)$,
       $$
         \P(X_{1} \in \{x_{1}\}, \ldots, X_{n} \in \{x_{n}\}) = \prod_{i=1}^{n}\P(X_{i} \in \{x_{i}\})
       $$
     - Supposons que pour tout $(x_{1}, \ldots, x_{n}) \in X_{1}(\Omega)\times \cdots \times X_{n}(\Omega)$, on a
       $$
         \P(X_{1} = x_{1}, \ldots, X_{n} = x_{n}) = \prod_{i=1}^{n}\P(X_{i} = x_{i})
       $$
       Soit $J$ une partie finie de $\icc{1,n}$ et $(B_{i})_{i \in J} \in \prod_{i \in J} \Part(X_{i}(\Omega))$. On a
       $$
         \P \left(\bigcap_{i \in J}(X_{i} \in B_{i})\right) = \P \left(\bigcap_{i \in \icc{1,n}}(X_{i} \in B_{i}')\right)
       $$
       avec $B_{i}' = \begin{cases}B_{i} \if i \in J \\ X_{i}(\Omega) \if i \in \icc{1,n}\end{cases}$.
       $$
         \begin{align*}
           \P \left(\bigcap_{i \in J}(X_{i} \in B_{i})\right) &= \P \left(\biguplus_{(x_{1}, \ldots, x_{n}) \in B_{1}' \times \cdots \times B_{n}'}(X_{1}=x_{1}) \cap \cdots \cap (X_{n} = x_{n})\right) \\
             &= \sum_{(x_{1}, \ldots, x_{n}) \in B_{1}' \times  \cdots \times  B_{n}'}\P(X_{1} = x_{1}, \ldots, X_{n}=x_{n}) \\
             &= \sum_{(x_{1}, \ldots, x_{n}) \in B_{1}' \times \cdots \times B_{n}'}\P(X_{1} = x_{1}) \times \cdots \times \P(X_{n}=x_{n}) \\
             &= \sum_{x_{1} \in B_{1}'}\P(X_{1} = x_{1}) \times \cdots \times \sum_{x_{n} \in B_{n}}\P(X_{n} = x_{n}) \\
             &= \P(X_{1} \in B_{1}') \times \cdots \times \P(X_{n} \in B_{n}') \\
             &= \prod_{i \in J}\P(X_{1} \in B_{i})
         \end{align*}
       $$
%

%prop
  Si $X_{1} \sim X_{2} \sim \cdots\sim X_{n}$ sont des variables aléatoires indépendantes suivant une loi de Bernoulli de paramètre $p \in \oo{0,1}$((Noté usuellement v.a.i.i.d (variables aléatoires indépendantes identiquement distribuées))). Alors
  $$
    X_{1} + \cdots + X_{n} \leadsto \B(n,p)
  $$
%

%eg
  1. Si $X\leadsto \mathscr P(\lambda)$, $Y\leadsto \mathscr P(\mu)$ et $X\indep Y$, alors $X + Y\leadsto \mathscr P(\lambda+\mu)$.
     ~
     Soit $k \in \N$.
     $$
       \begin{align*}
         \P(X+Y=k) &= \P \left(\biguplus_{x \in X(\Omega)}(X=x) \cap (Y = k-x)\right) \\
         &= \sum_{x \in X(\Omega)}\P(X=x,Y=k-x) \\
         &\ueq{X\indep Y} \sum_{x \in X(\Omega)}\P(X=x)\P(Y=k-x) \\
         &= \sum_{\ell=0}^{k} e^{-\lambda} \frac{\lambda^{k}}{\ell!} \times e^{-\mu}\frac{\mu^{k-\ell}}{(k-\ell)!} \\
         &= \frac{e^{-(\lambda+\mu)}}{k!}\sum_{\ell=0}^{k}\binom{\ell}{k}\lambda^{\ell}\mu^{k-\ell} \\
         &= \frac{e^{-(\lambda+\mu)}}{k!}(\lambda + \mu)^{k}
       \end{align*}
     $$
     donc $X + Y \leadsto \mathscr P(\lambda + \mu)$.
  2. Si $X\leadsto \mathscr G(p) = \mathscr P(1,p)$, $Y\leadsto \mathscr G(p)$ et $X\indep Y$, alors $X + Y \leadsto \mathscr P(1,p)$ (loi de Pascal de paramètres 1 et $p$).
     ~
     Si $X(\Omega) = \N^{*} = Y(\Omega)$, on a $(X + Y)(\Omega) \subset \ico{2,+\infty}$. Soit $k \in \ico{2,+\infty}$.
     $$
       \begin{align*}
         \P(X+Y = k) &= \P \left(\biguplus_{\ell=1}^{k-1}(X=\ell) \cap (Y = k-\ell)\right) \\
         &\ueq{X\indep Y} \sum_{\ell=1}^{k-1}\P(X=\ell)\P(Y = k-\ell) \\
         &= \sum_{\ell=1}^{k-1}p(1-p)^{\ell-1} \times p(1-p)^{k-\ell-1} \\
         &= p^{2} \sum_{\ell=1}^{k-1}(1-p)^{k-2} = (k-1)p^{2}(1-p)^{k-2}
       \end{align*}
     $$
     donc $X+Y \leadsto \mathscr P(1,p)$.
%

%prop
  1. Soient $X: (\Omega,\Tribu,\P) \longrightarrow E$ et $X': (\Omega,\Tribu,\P) \longrightarrow E'$ deux variables aléatoires discrètes. Soient $f: X(\Omega) \longrightarrow F$ et $g: X'(\Omega) \longrightarrow F'$ deux applications. On a
     $$
       X\indep Y \implies f(X)\indep g(Y)
     $$
  2. _(Lemme des coalitions)_ Soient $X_{1}, \ldots, X_{n}$ une suite finie de variables aléatoires toutes définies sur $(\Omega,\Tribu,\P)$ **indépendantes**. Soit $(I_{k})_{k \in \icc{1,p}}$ une partition de $\icc{1,n}$. Alors
     - les vecteurs aléatoires $(X_{i})_{i \in I_{1}}, \ldots, (X_{i})_{i \in I_{p}}$ sont indépendants.
     - les variables aléatoires $f_{1}\big((X_{i})_{i \in I_{1}}\big), \ldots, f_{p} \big((X_{i})_{i \in I_{p}}\big)$ sont indépendants où $f_{k}: (X_{i})_{i \in I_{k}}(\Omega) \longrightarrow E_{k}$ est une application.
%

%proof
  2. Supposons $X_{1}, \ldots, X_{n}$ indépendants. Soit $\big((x_{i})_{i \in I}, \ldots, (x_{i})_{i \in I_{p}}\big) \in (X_{i})_{i \in I_{1}}(\Omega) \times \cdots \times (X_{i})_{i \in I_{p}}(\Omega)$.
     $$
       \begin{align*}
         \P \Big((X_{i})_{i \in I_{1}} = (x_{i})_{i \in I_{1}}, \ldots, (X_{i})_{i \in I_{p}} = (x_{i})_{i \in I_{p}}\Big) &= \P(X_{1} = x_{1}, \ldots, X_{n} = x_{n}) \\
        &= \prod_{i=1}^{n}\P(X_{i} = x_{i}) \\
        &= \prod_{k=1}^{p} \left(\prod_{i \in I_{k}}\P (X_{i} = x_{i})\right)
       \end{align*}
     $$
     Comme $X_{1}, \ldots, X_{n}$ sont indépendantes, $(X_{i})_{i \in I_{k}}$ est une famille de variables aléatoires indépendantes d’où
     $$
       \prod_{i \in I_{k}}\P(X_{i} = x_{i}) = \P \left(\bigcap_{i \in I_{k}}(X_{i} = x_{i})\right)
     $$
     d’où le résultat.
     - Soient $\alpha_{1} \in f_{1} \big((X_{i})_{i \in I_{1}}\big)(\Omega)$, …, $\alpha_{p} \in f_{p} \big((X_{i})_{i \in I_{p}}\big)(\Omega)$
       $$
         \begin{align*}
           \P \Big(f_{1}\big((X_{i})_{i \in I_{1}}\big) = \alpha_{1}, \ldots, f_{p} \big((X_{i})_{i \in I_{p}}\big)(\Omega) = \alpha_{p}\Big) &= \P \big((X_{i})_{i \in I_{1}} \in f^{-1}_{1}(\{\alpha_{1}\}), \ldots, (X_{i})_{i \in I_{p}} \in f_{p}^{-1}(\{\alpha_{p}\})\big) \\
            &= \prod_{k=1}^{p}\P \Big((X_{i})_{i \in I_{k}} \in f_{k}^{-1}\big(\{\alpha_{k}\}\big)\Big) \\
            &= \prod_{k=1}^{p} \P \Big(f_{k} \big((X_{i})_{i \in I_{k}}\big) = \alpha_{k}\Big)
         \end{align*}
       $$
%

%eg
  1. Si $X$ et $Y$ sont deux variables aléatoires discrètes réelles indépendantes, alors $X^{k}$ et $Y^{\ell}$ sont indépendantes pour tout $(k,\ell) \in \N^{2}$.
  2. Si $X_{1}, \ldots, X_{2n}$ est une suite de variables aléatoires discrètes réelles **indépendantes**, alors $X_{1} + X_{2n}, X_{2} + Y_{2n-1} , \ldots, X_{n}+X_{n+1}$ est une suite de variables aléatoires indépendantes.
  3. Si $(X_{n})_{n \in \N^{*}}$ est une suite de variables aléatoires **indépendantes** telle que $\forall n \in \N^{*}, X_{n} \leadsto \mathscr P(\lambda_{n})$ avec $(\lambda_{n})_{n \in \N^{*}}$ une bamille de réels strictement positifs, alors
     $$
       \forall k \in \N^{*}, X_{1} + \cdots + X_{k} \leadsto \mathscr P(\lambda_{1} + \cdots + \lambda_{k})
     $$
     ~
     Considérons la propriété définie pour $k \in \N^{*}$ par
     $$
       \Prop(k): « X_{1} + \cdots + X_{k} \leadsto \mathscr P(\lambda_{1} + \cdots + \lambda_{k}) »
     $$
     - $\Prop(1)$ est vraie
     - Supposons $\Prop(k)$ vraie pour $k \in \N^{*}$. Les variables $X_{1}, \ldots, X_{k+1}$ sont indépendantes donc d’après le _lemme des coalitions_, $(X_{1} + \cdots + X_{k})\indep X_{k+1}$ or $X_{1} + \cdots + X_{k}\leadsto \mathscr P(\lambda_{1} + \cdots + \lambda_{k})$ et $X_{k+1}\leadsto \mathscr P(\lambda_{k+1})$ donc $\Prop(2)$ permet d’affirmer~:
       $$
         (X_{1} + \cdots + X_{k})+X_{k+1} \leadsto \mathscr P(\lambda_{1} + \cdots + \lambda_{k+1})
       $$
%

## Espérance d’une variable aléatoire complexe

Soit $(\Omega,\Tribu,\P)$ un espace probabilisé.

%def
  Soit $X: \Omega \longrightarrow \R_{+} \cup \{+\infty\}$ une variable aléatoire «positive». On appelle _espérance de $X$_ le «réel»
  $$
    E(X) = \sum_{x \in X(\Omega)}x\P(X=x) \in \bar{\R_{+}}
  $$
  avec les conventions
  ~ $(+\infty)\times \lambda = +\infty$ si $\lambda \in \oc{0,1}$  ~ $(+\infty) \times 0 = 0$
%

%eg
  - Si $X(\Omega) \subset A \subset \big(\R_{+} \cup \{+\infty\}\big)$, alors
    $$
      E(X) = \sum_{x \in A}x\P(X=x) = \sum_{x \in \R_{+} \cup \{+\infty\}}x \P(X=x)
    $$
  - Si $X$ et $Y$ sont deux variables aléatoires «positives» suivant la même loi, alors $E(X) = E(Y) \in \bar{\R_{+}}$. En effet,
    $$
      E(X) =\sum_{x \in X(\Omega)}x \P(X=x) = \sum_{x \in X (\Omega) \cup Y(\Omega)}x\P(X=x) = \sum_{x \in X(\Omega) \cup Y(\Omega)}y\P(Y=x) = E(Y)
    $$
  - %callout
      Soit $A$ un événement. $\1_{A}$ est une variable aléatoire **positive** et $\1_{A}(\Omega) \subset \{0,1\}$.
      $$
        E(\1_{A}) = \sum_{x \in \{0,1\}}x\P(\1_{A}=x) = \P(\1_{A} = 1) = \P(A)
      $$
      donc
      $$
        \boxed{\forall A \in \Tribu, \P(A) = E(\1_{A})}
      $$
    %
%

%prop
  Soit $X: (\Omega,\Tribu,\P) \longrightarrow \R_{+}$ une variable aléatoire discrète, $p \in \oo{0,1}$ et $\lambda > 0$.
  1. si $X \leadsto \B(p)$, alors $E(X) = p$.
  2. si $X \leadsto \B(n,p)$, alors $E(X) = np$.
  3. si $X \leadsto \mathscr P(\lambda)$, alors $E(X) = \lambda$.
%

%proof
  1. $X \sim \1_{(X=1)}$ (on écrit parfois $X = \1_{(X=1)}$) donc $E(X) = E(\1_{(X=1)}) = \P(X=1) = p$.
  2. Soit $X \leadsto \B(n,p)$.
     $$
       \begin{align*}
         E(X) &= \sum_{x \in X(\Omega)}x\P(X=x)  \\
         &= \sum_{k=0}^{n}k\P(X=k) & \text{car } \forall x \in \R_{+} \setminus \icc{0,n}, \P(X=x) = 0 \\
         &= \sum_{k=\stress 1}^{n}k \binom{n}{k}p^{k}(1-p)^{n-k} \\
         &= \sum_{k=1}^{n}k \frac{n}{k}\binom{n-1}{k-1}p^{k}(1-p)^{n-k} \\
         &= n \sum_{k=1}^{n} \binom{n-1}{k-1}p^{k}(1-p)^{n-k} \\
         &= n \sum_{k=0}^{n-1} \binom{n-1}{k}p^{k+1}(1-p)^{n-1-k} \\
         &= np \underbrace{\sum_{k=0}^{n-1} \binom{n-1}{k}p^{k}(1-p)^{n-1-k}}_{(p+1-p)^{n-1}} \\
         &= np
       \end{align*}
     $$
  3. Soit $X \leadsto \mathscr P(\lambda)$ une variable aléatoire discrète positive.
     $$
       \begin{align*}
         E(X) &= \sum_{k \in \N} k\P(X=k) \\
         &= \sum_{k \in \N}k e^{-\lambda}\frac{\lambda^{k}}{k!} \\
         &= \sum_{k \in \N^{*}}e^{-\lambda}\frac{\lambda^{k}}{(k-1)!} \\
         &= \lambda e^{-\lambda} \sum_{k \in \N^{*}} \frac{\lambda^{k-1}}{(k-1)!} \\
         &= \lambda e^{-\lambda}e^{\lambda} = \lambda
       \end{align*}
     $$
%

%prop
  Si $X: \Omega \longrightarrow \N \cup \{+\infty\}$ est une variable aléatoire entière «positive», alors
  $$
    E(X) = \sum_{n \in \N^{*}}\P(X \geq n) \in \bar{\R_{+}}
  $$
%

%proof
  - Si $X(\Omega) \subset \N$, alors
    $$
      \begin{align*}
        E(X) &\defeq \sum_{k \in \N}k\P(X=k) \\
        &= \sum_{k \in \N^{*}}k\P(X=k) \\
        &= \sum_{k \in \N^{*}} \left(\sum_{\ell=1}^{k}\P(X=k)\right) \\
        &= \sum_{\ell \in \N^{*}} \left(\sum_{k \in \ico{\ell,+\infty}} \P(X=k)\right) & \text{dans }\bar{\R_{+}} \\
        &= \sum_{\ell \in \N^{*}}\P(X \geq \ell) & \text{par $\sigma$-additivité}
      \end{align*}
    $$
  - Sinon, $(+\infty) \in X(\Omega)$.
    - si $\P(X=+\infty) = 0$, alors $(+\infty) \times \P(X=+\infty) = 0$ et
      $$
        E(X) = \sum_{k \in \N}k\P(X=k) = \sum_{\ell \in \N^{*}}\P(X \in \ico{\ell,+\infty}) = \sum_{\ell \in \N^{*}}\P(X \in \icc{\ell,+\infty}) = \sum_{\ell \in \N^{*}}\P(X \geq \ell)
      $$
    - sinon, $(+\infty)\P(X=+\infty) = +\infty$ donc $E(X) \geq (+\infty)\P(X=+\infty)$ donc $E(X) = +\infty$ et
      $$
        \sum_{n \in \N^{*}}\P(X \geq n) \geq \sum_{n \in \N^{*}}\P(X \in \icc{n,+\infty}) \geq \sum_{n \in \N^{*}}\P(X=+\infty) = +\infty
      $$
%

%prop
  Si $X$ est une variable aléatoire suivant une loi géométrique $\mathscr G(p)$ et est telle que $X(\Omega) \subset \bar{\R_{+}}$, alors $E(X) = \frac{1}{p}$.
%

%proof
  On a $X \sim Y$ avec $Y$ suivant la loi $\mathscr G(p)$ telle que $Y(\Omega) = \N^{*}$, donc
  $$
    \begin{align*}
      E(X) &= E(Y) \\
        &= \sum_{n \in \N^{*}}\P(Y \geq n) \\
        &= \sum_{n \in \N^{*}} \P(Y > n-1) \\
        &= \sum_{n \in \N^{*}} (1-p)^{n-1} \\
        &= \sum_{n \in \N} (1-p)^{n} \\
        &= \frac{1}{1-(1-p)} = \frac{1}{p}
    \end{align*}
  $$
%

%def
  Soit $X: (\Omega,\Tribu,\P) \longrightarrow \C$ une variable aléatoire discrète complexe. On dit que $X$ est d’_espérance finie_ si la famille $\big(x\P(X=x)\big)_{x \in X(\Omega)}$ est sommable et on écrit $X \in L^{1}(\Omega,\Tribu,\P)$, $X \in L^{1}(\Omega)$ ou si aucune confusion n’est possible, $X \in L^{1}$.
  $$
    X \in L^{1} \overset{\text{def}}\iff \big(x\P(X=x)\big)_{i \in X(\Omega)} \in \ell^{1}\big(X(\Omega)\big)
  $$
  On pose alors
  $$
    E(X) = \sum_{x \in X(\Omega)}x\P(X=x) \in \C
  $$
  On dit que $X$ est _centrée_ si elle est d’espérance nulle.
%

%eg
  - Si $X(\Omega)$ est fini, alors $X \in L^{1}(\Omega)$
  - Si $X\leadsto \mathscr G(p)$ ou $X\leadsto \mathscr P(\lambda)$, alors $X \in L^{1}(\Omega)$
%

%lemma
  Si $X$ est une variable aléatoire discrète bornée $\P$-presque sûrement, alors $X$ est d’espérence finie.
%

%proof
  Il existe $M \in \R_{+}$ tel que $\P(\abs{X} \leq M) = 1$. On calcule dans $\bar{\R_{+}}$~:
  $$
    \sum_{x \in X(\Omega)}\abs{x}\P(X=x) = \sum_{\substack{x \in X(\Omega) \\ \abs{x} \leq M}} \abs{x}\P(X=x) + \sum_{\substack{x \in X(\Omega) \\ \abs{x} > M}}\abs{x} \P(X=x)
  $$
  si $\abs{x} > M$, alors $(X=x) \subset (\abs{X} > M)$ d’où $\P(X=x) \leq 1-\P(\abs{X} \leq M) = 0$ donc $\P(X=x) = 0$. Ainsi,
  $$
    \sum_{x \in X(\Omega)}\abs{x}\P(X=x) = \sum_{\substack{x \in X(\Omega) \\ \abs{x} \leq M}}\abs{x} \P(X=x) \leq M \sum_{x \in X(\Omega)}\P(X=x) = M \times 1 = M < +\infty
  $$
%

%eg
  Soit $X \leadsto \mathscr G(p)$ avec $X(\Omega) \subset \N^{*}$. Posons $Y = \frac{(-1)^{X}}{X}$. On a $ey = f(X)$ avec $f: \applic{\N^{*}}{\R}{n}{\frac{(-1)^{n}}{n}}$ donc $Y \in L^{0}(\Omega)$. On a $\abs{Y} f \frac{1}{X} \leq 1$ i.e. $Y$ est bornée, donc $Y \in L^{1}(\Omega)$.
  $$
    Y(\Omega) = \left\{\frac{(-1)^{n}}{n} \where n \in \N^{*}\right\}
  $$
  On a
  $$
    E(Y) = \sum_{y \in Y(\Omega)}y \P(Y=y) = \sum_{n \in \N^{*}}\frac{(-1)^{n}}{n}\P \left(Y = \frac{(-1)^{n}}{n}\right)
  $$
  or $\left(Y=\frac{(-1)^{n}}{n}\right) = (X=n)$  donc
  $$
    \begin{align*}
      E(X) &= \sum_{n \in \N^{*}}\frac{(-1)^{n}}{n} \P(X=n) \\
          &= \sum_{n \in \N^{*}}\frac{(-1)^{n}}{n}p(1-p)^{n-1} \\
          &= \frac{p}{1-p}(-1) \sum_{n \in \N^{*}}(-1)^{n-1}\frac{(1-p)^{n}}{n} \\
    \end{align*}
  $$
  or $1-p \in \oo{0,1}$ et $\forall x \in \oo{-1,1}, \ln(1+x) = \sum_{n=1}^{+\infty}(-1)^{n-1\frac{x^{n}}{n}}$ d’où
  $$
    E(Y) = \frac{p}{p-1}\ln(1+1-p) = \frac{p\ln(2-p)}{p-1} < 0
  $$
%

%eg
  Soit $X \leadsto \mathscr P(\lambda)$ telle que $X(\Omega)= \N$. Soit $N \in \N^{*}$. Posons $Y = e^{\frac{2i \pi X}{N}} = g(X)$ avec $g: \applic{\N}{\C}{k}{e^{\frac{2i \pi k}{N}}}$ donc $Y \in L^{0}(\Omega)$. De plus, $\abs{Y} = 1$ donc $Y \in L^{1}(\Omega)$. On a $Y(\Omega) = \left\{e^{\frac{2ik \pi}{N}} \where k \in \N\right\} = \U_{N}$ donc
  $$
    E(Y) = \sum_{\omega \in \U_{N}} \omega\P(Y=\omega)
  $$
  Pour $k \in \N$,
  $$
    \begin{align*}
      \P \left(Y = e^{\frac{2ik \pi}{N}}\right) &=\P \left(e^{\frac{2i \pi X}{N}} = e^{\frac{2ik \pi}{N}}\right) \\
      &= \P \big(X \in k+N\N\big)
    \end{align*}
  $$
  donc
  $$
    E(Y) = \sum_{k=0}^{N-1} e^{\frac{2i \pi k}{N}} \left(\sum_{\ell \in k+N\N} e^{-\lambda} \frac{\lambda^{\ell}}{\ell!}\right)
  $$
  Il faut un autre résultat: le théorème de transfert.
%

%thm Théorème de Transfert
  _(Permet le calcul de l’espérance de $f(X)$ à partir de la loi de $X$ et non de celle de $f(X)$)._ Soit $X : (\Omega, \Tribu,\P) \longrightarrow E$ une variable aléatoire discrète. Soit $f: X(\Omega) \longrightarrow \C$ une application. Alors
  1. _(Théorème de Transfert (T.T))_
     $$
       f(X) \in L^{1}(\Omega) \iff \Big(f(x) \P(X=x)\Big)_{x \in X(\Omega)} \in \ell^{1} \big(X(\Omega)\big)
     $$
     le membre de gauche étant par définition équivalent à $\Big(y\P(f(X) = y)\Big)_{y \in f(X)(\Omega)} \in \ell^{1} \big(f(X)(\Omega)\big)$.
  2. _(Formule de transfert)_ Si $f(X) \in L^{1}(\Omega)$, alors
     $$
       \underbrace{E (f(X))}_{\sum_{k\in f(X)(\Omega)}y\P(f(X) = y)} = \sum_{x \in X(\Omega)} f(x) \P(X=x)
     $$
%

%custom Application
  Soit $Y = e^{\frac{2i \pi X}{N}} = g(X)$ où $X \leadsto \mathscr P(\lambda)$. $\abs{Y} = 1$ donc $Y = g(X) \in L^{1}(\Omega)$. Ainsi, d’après la formule de transfert,
  $$
    \begin{align*}
      E(Y) &= \sum_{k \in \N} e^{\frac{2i \pi k}{N}} \P(X=k) = \sum_{k \in \N}e^{\frac{2i \pi k}{N}}e^{-\lambda} \frac{\lambda^{k}}{k!} \\
      &= e^{-\lambda}e^{\lambda e^{\frac{2i \pi}{N}}} \\
      &= \exp \left(\lambda\left(e^{\frac{2i \pi}{N}}-1\right)\right)
    \end{align*}
  $$
%

%proof
  On a $f(X): \Omega \xrightarrow{X} X(\Omega) \xrightarrow{f} \C$. Posons $D = f \big(X(\Omega)\big)$ au plus dénombrable. On se place dans $\bar{\R_{+}}$~:
  $$
    \begin{align*}
      \sum_{y \in D}\abs{y}\P \big(f(X) = y\big) &= \sum_{y \in D}\abs{y}\P \big(X \in f^{-1}(\{y\})\big) \\
      &= \sum_{y \in D}\abs{y}\sum_{x \in f^{-1}(\{y\})}\P(X=x) \\
      &= \sum_{y \in D} \left(\sum_{x \in f^{-1}(\{y\})}\abs{f(x)}\P(X=x)\right)
    \end{align*}
  $$
  On a $X(\Omega) = \biguplus_{y \in D}f^{-1}(\{y\})$ donc
  $$
    \sum_{y \in D}\abs{y}\P \big(f(X) = y\big) = \sum_{x \in X(\Omega)}\abs{f(x)}\P(X=x)
  $$
  donc
  $$
    \Big(y \P \big(f(X) = y\big)\Big)_{y \in D}\in \ell^{1}(D) \iff \Big(f(x)\P(X=1)\Big)_{x \in X(\Omega)} \in \ell^{1}(X(\Omega))
  $$
  et si $f(X) \in L^{1}$, alors
  $$
    E \big(f(X)\big) = \sum_{y \in D}y\P \big(f(X) = y\big) = \sum_{y \in D}y \P \big(X \in f^{-1}(\{y\})\big) = \cdots = \sum_{x \in X(\Omega)f(x) \P(X=1)}
  $$
%

%eg
  Soit $X \leadsto \mathscr \P(\lambda)$. Posons $Y = \frac{(-2)^{X}}{X+1} = f(X)$ avec $f: \applic{\N}{\R}{n}{\frac{(-2)^{n}}{n+1}}$. $Y$ est bien une variable aléatoire.
  $$
    Y \in L^{1}(\Omega) \iff \left(\frac{(-2)^{n}}{n+1}\P(X=n)\right)_{n \in \N} \in \ell^{1}(\N)
  $$
  or
  $$
    \abs{\frac{(-2)^{n}}{n+1}e^{-\lambda} \frac{\lambda^{n}}{n!}} = \frac{e^{-\lambda} (2 \lambda)^{n+1}}{2 \lambda \cdot (n+1)!}
  $$
  et $\left(\frac{(2 \lambda)^{n+1}}{n+1} e^{-\lambda} \frac{\lambda^{n}}{n!}\right)_{n \in \N} \in \ell^{1}(\N)$ donc $Y \in L^{1}(\Omega)$ (_théorème de transfert_). D’après la _formule de transfert_, on a
  $$
    \begin{align*}
      E(Y) &= \sum_{n \in \N}\frac{(-2)^{n}}{n+1}\P(X=n) \\
            &= \sum_{n \in \N}\frac{(-2)^{n}}{n+1}e^{-\lambda}\frac{\lambda^{n}}{n!} \\
            &= \sum_{n \in \N}\frac{e-\lambda}{(-2 \lambda)}\frac{(-2 \lambda)^{n+1}}{(n+1)!} \\
            &= \frac{e^{-\lambda}}{(-2 \lambda)}\big(e^{-2 \lambda}-1\big) \\
            &= \frac{e^{-2 \lambda}}{2 \lambda}\big(e^{\lambda} - e^{-\lambda}\big) \\
            &= e^{-2 \lambda} \frac{\sh \lambda}{\lambda}
    \end{align*}
  $$
%

%rem
  Si $X: (\Omega, \Tribu,\P) \longrightarrow E$ est une variable aléatoire discrète et si $f: X(\Omega) \longrightarrow \R_{+}$ est une application positive, alors $f(X)$ est une variable aléatoire discrète positive et dans $\bar{\R_{+}}$. De plus, on a la _formule de transfert_ dans $\bar{\R_{+}}$~:
  $$
    E \big(f(X)\big) = \sum_{x \in X(\Omega)}f(x)\P(X=x)
  $$
  En effet, en posant $D \in f(X)(\Omega)$,
  $$
    E \big(f(X)\big) \underset{\bar{\R_{+}}}\defeq \sum_{y \in D}y \P \big(f(X) = y\big) \ueq{\bar{\R_{+}}} \sum_{x \in X(\Omega)}f(x) \P(X=x)
  $$
%

%prop
  $L^{0}(\Omega)$ est l’ensemble des variable aléatoires discrètes réelles ou complexes.
  1. Si $X \in \stress{L^{1}(\Omega)}$ suit la même loi que $Y \in \stress{L^{0}(\Omega')}$, alors $Y \in L^{1}(\Omega')$ et $E(X) = E(Y)$.
  2. Pour $X \in L^{0}(\Omega)$, on a
     $$
       X \in L^{1} \iff \abs{X} \in L^{1}
     $$
     et pour $X \in L^{1}$, on a
     $$
       \abs{E(X)} \leq E(\abs{X})
     $$
     _(inégalité triangulaire)_
  3. Si $X: \Omega \longrightarrow \R_{+}$ est une variable aléatoire discrète positive d’espérence finie et $Y \in L^{0}(\Omega)$ est telle que $\abs{Y} \leq X$, alors $Y \in L^{1}(\Omega)$.
  4. $L^{1}(\Omega)$ est un sous-espace vectoriel de $(\Func(\Omega,\K), +, \cdot)$ et $E: L^{1}(\Omega) \longrightarrow \K$ est une forme linéaire sur $L^{1}(\Omega)$.
  5. Si $X$ et $Y$ sont deux variables aléatoires discrètes de $L^{1}(\Omega)$ telles que $X \leq Y$, alors $E(X) \leq E(Y)$ _(croissance de l’espérence)_.
  6. Si $X: \Omega \longrightarrow \R_{+}$ est une variable aléatoire discrète positive, alors
     ~ $E(X) \in \cc{0,+\infty}$  ~ $E(X) = 0 \iff X=0 \ps$
%

%rem
  On peut ajouter «$\P$-presque sûrement» à toutes les relation de comparaison dans les propriétés ci-dessus.
  En réalité, $L^{1}(\Omega)$ est défini comme l’ensemble $L^{1}(\Omega)$ défini plus haut quotienté par la relation d’égalité $\P$-presque sûre.
%

%rem
  - Si $X: \Omega \longrightarrow \C$ et $Y: \Omega \longrightarrow \C$ deux variables aléatoires discrètes. $X = Y \ps$ signifie $\P(X=Y) = 1$.
  - Si $X: \Omega \longrightarrow \C$ et $Y : \Omega' \longrightarrow \C$ sont deux variables aléatoires discrètes, on a
    $$
      X \sim Y \iff \forall x \in X(\Omega) \cup Y(\Omega'), \P(X=x) = \P'(Y=x)
    $$
    Dans le cas où $\Omega' = \Omega$,
    $$
      X=Y \ps \implies X\sim Y
    $$
    En effet, en posant $\Omega_{1} = \Big\{\omega \in \Omega \where X(\omega) = Y(\omega)\Big\}$, on a $\P(\Omega_{1}) = 1$. Soit $x \in X(\Omega) \cap Y(\Omega)$
    $$
      \P(X=x) = \P \big((X=x) \cap \Omega_{1}\big) + \underbrace{\P \big((X=x) \cap \bar{\Omega_{1}}\big)}_{\leq \P(\bar{\Omega_{1}}) = 0}
    $$
    donc
    $$
      \P(X=x) = \P \big((X=x) \cap \Omega_{1}\big) = \P \big((Y=x) \cap \Omega_{1}\big) = \P(Y=x)
    $$

%

%proof
  1. Soient $X \in L^{0}(\Omega)$ suivant la même loi que $Y \in L^{0}(\Omega')$.
     $$
       \sum_{x \in X(\Omega)}\abs{x}\P(X=x) = \sum_{x \in X(\Omega) \cup Y(\Omega')}\abs{x}\P(X=x) = \sum_{x \in X(\Omega) \cup Y(\Omega')}\abs{x}\P'(Y=x) = \sum_{x \in Y(\Omega')}\abs{x}\P'(Y=x)
     $$
     donc $X \in L^{1}(\Omega) \iff Y \in L^{1}(\Omega')$ et si $X \in L^{1}(\Omega)$, alors
     $$
       E(X) = \sum_{x \in X(\Omega)}x\P(X=x) = \sum_{x \in Y(\Omega')}x\P'(Y=x) = E(Y)
     $$
  2. Soit $X \in L^{0}(\Omega)$.
     $$
       \begin{align*}
         X \in L^{1}(\Omega) &\defiff \Big(x\P(X=x)\Big)_{x \in X(\Omega)} \in \ell^{1}\big(X(\Omega)\big) \\
         & \defiff \Big(\abs{x}\P(X=x)\Big)_{x \in X(\Omega)} \in \ell^{1}(X(\Omega)) \\
         & \oiff{T.T} \abs{X} \in L^{1}(\Omega)
       \end{align*}
     $$
     et pour $X \in L^{1}(\Omega)$,
     $$
       \abs{E(X)} = \abs{\sum_{x \in X(\Omega)} x\P(X=x)} \leq \sum_{x \in X(\Omega)}\abs{x}\P(X=x) \oeq{F.T}E(\abs{X})
     $$
  3. Soit $X$ une variable aléatoire discrète d’espérance finie et $Y \in L^{0}(\Omega)$ telle que $\abs{Y} \leq X$. Alors
     $$
       \sum_{y \in Y(\Omega)} \abs{y}\P(Y=y) = \sum_{y \in Y(\Omega)}\abs{y} \left(\sum_{x \in X(\Omega)}\P(Y =y, X=x)\right)
     $$
     car $(X=x)_{x \in X(\Omega)}$ est un système complet d’événements d’où
     $$
       \sum_{y \in Y(\Omega)}\abs{y}\P(Y=y) = \sum_{(x,y) \in X(\Omega)\times Y(\Omega)} \abs{y}\P(X=x,Y=y) = \sum_{\substack{(x,y) \in X(\Omega) \times Y(\Omega)\\\abs{y} \leq x}}\cdots  + \sum_{\substack{(x,y) \in X(\Omega)\times Y(\Omega)\\ \abs{y} > x}}\cdots
     $$
     or si $\abs{y} > x$, alors $(X=x) \cap (Y=x) \subset (\abs{Y} > X)$
     %offprog
       or $\abs{Y} \leq X \ps$ donc $\P \big(\abs{Y} > X\big) = 0$ d’où $\P(X=x, Y=y) = 0$
     %
     d’où
     $$
       \begin{align*}
         \sum_{y \in Y(\Omega)} \abs{y}\P(Y=x) &= \sum_{(x,y) \in X(\Omega)\times Y(\Omega)}\abs{y}\P(X=x,Y=y) \\
         &\leq \sum_{x \in X(\Omega)}\abs{x}\sum_{y \in Y(\Omega)}\P(X=x,Y=y) = \sum_{x \in X(\Omega)}\abs{x}\P(X=x) < +\infty
       \end{align*}
     $$
  4. - $\1_{\emptyset} \in L^{1}(\Omega)$.
     - Soient $X$ et $Y$ deux variables aléatoires discrètes de $L^{1}$. Soient $(\lambda, \mu) \in \K^{2}$. On a $\lambda X  + \mu Y = f(X,Y)$ avec $f: \applic{\K^{2} }{\K}{(x,y)}{\lambda x + \mu y}$.
       $$
         \lambda X + \mu Y \in L^{1} \oiff{T.T} \Big((\lambda x + \mu y) \P(X=x,Y=y)\Big)_{(x,y) \in (X,Y)(\Omega)} \in \ell^{1} \big((X,Y)(\Omega)\big) \\
       $$
       On se prace dans $\bar{\R_{+}}$~:
       $$
         \begin{align*}
           \sum_{(x,y) \in (X,Y)(\Omega)} \abs{\lambda x + \mu y} \P(X=x,Y=y) &= \sum_{(x,y) \in X(\Omega) \times Y(\Omega)} \abs{\lambda x + \mu y} \P(X=x,Y=y) \\
           &\leq \sum_{x \in X(\Omega)} \abs{\lambda}\abs{x} \left(\sum_{y \in Y(\Omega)}\P(X=x,Y=y)\right) + \sum_{y \in Y(\Omega)}\abs{\mu}\abs{y} \left(\sum_{x \in X(\Omega)}\P(X=x,Y=y)\right) \\
           &= \abs{\lambda} \sum_{x \in X(\Omega)} \abs{x}\P(X=x) + \abs{\mu} \sum_{y \in Y(\Omega)}\abs{y}\P(Y=y) < +\infty
         \end{align*}
       $$
       D’après le _théorème de transfert_, $\lambda X + \mu Y \in L^{1}(\Omega)$ et
       $$
         \begin{align*}
           E(\lambda X + \mu Y) &\oeq{F.T} \sum_{(x,y) \in (X,Y)(\Omega)} (\lambda x + \mu y)\P(X=x,Y=y)\\
           &= \sum_{(x,y) \in X(\Omega)\times Y(\Omega)}\cdots \\
           &= \lambda E (X) + \mu E(Y)
         \end{align*}
       $$
  5. Soient $X$ et $Y$ deux variables aléatoires discrètes d’espérances finies telles que $X \leq Y$ $\P$-presque sûrement. On a $Y - X \geq 0 \ps$ et $Y-X \in L^{1}$ d’où $E(Y-X) \in \co{0,+\infty}$. Par linéarité, $E(Y) - E(X) \geq 0$ d’où $E(X) \leq E(X)$.
  6. Soit $X: \Omega \longrightarrow \R_{+}$ une variable aléatoire discrète positive. ALors $E(X) \in \cc{0,+\infty}$ et
     $$
       \begin{align*}
         E(X) = 0 &\iff \sum_{x \in X(\Omega)}\underbrace{x\P(X=x)}_{\geq 0} = 0\\
         &\iff \forall x \in X(\Omega), x\P(X=x) = 0 \\
         &\iff \forall x \in X(\Omega) \cap \R_{+}^{*}, \P(X=x) = 0\\
         &\iff \P(X=0) = 1
       \end{align*}
     $$
%

%eg
  - Soit $a \in \K$.
    $$
      E(``a") = E(a \1_{\Omega}) = aE(\1_{\Omega}) = a\P(\Omega) = a
    $$
    Soit $X \in L^{1}$. Soit $(a,b) \in \K^{2}$. Par linéarité,
    $$
      E(aX+b) = a E(X) + b
    $$
  - La variable aléatoire _centrée_ associée à $X \in L^{1}$ est
    $$
      \tilde{X} = X-E(X) = X-E(X)1_{\Omega}
    $$
    Elle est d’espérance nulle.
  - Soient $A_{1}, \ldots, A_{n}$ des événements. Posons $B = \bigcup_{i \in 1}^{n}A_{i}$. On a $\bar{B} = \bigcap_{i=1}^{n}\bar{A_{i}}$ et
    $$
      \1_{\bar{B}} = \1_{\bigcap_{i=1}^n\bar{A_{i}}} = \prod_{i=1}^{n}\1_{\bar{A_{i}}}
    $$
    $$
      \begin{align*}
        \1_{\Omega} - \1_{B} &= \prod_{i=1}^{n}(\1_{\Omega} - \1_{A_{i}}) \\
        &= \1_{\Omega} - \sum_{i=1}^{n}\1_{A_{i}} + \sum_{1 \leq i < j \leq n} \1_{A_{i}} \1_{A_{j}} + (-1)^{n} \1_{A_{1}} \times \cdots \times \1_{A_{n}}
      \end{align*}
    $$
    d’où
    $$
      \1_{B} = \sum_{i=1}^{n}\1_{A_{i}} - \sum_{1 \leq i < j \leq n}\1_{A_{i} \cap A_{j}} + \cdots + (-1)^{n-1}\1_{A_{1} \cap  \cdots \cap  A_{n}}
    $$
    Par linéarité de l’espérance, $E(\1_{A}) = \P(A)$ pour tout événement $A$, donc
    $$
      \P(B) = \sum_{i=1}^{n}\P(A_{i}) - \sum_{1 \leq i < j \leq n} \P(A_{i} \cap A_{j}) + \cdots + (-1)^{n-1}\P(A_{1} \cap  \cdots \cap A_{n})
    $$
    On retrouve enfin la _formule du crible de Poincaré_~:
    $$
      \P \left(\bigcup_{i=1}^{n}B_{i}\right) = \sum_{k=1}^{n}(-1)^{k-1} \left(\sum_{\substack{I \in \Part(\icc{1,n}) \\ \abs{I} =k}}\P \left(\bigcap_{i \in I} A_{i}\right)\right)
    $$
%

%rem
  - Soit $X \leadsto Z(3)$ positive. Dans $\bar{\R_{+}}$, on a
    $$
      E(X) = \sum_{k \in \N^{*}}k \frac{1}{\zeta(3)k^{3}} = \frac{\zeta(2)}{\zeta(3)} < +\infty
    $$
    donc $X \in L^{1}(\Omega)$. Or $X^{2} \geq 0$ et
    $$
      E(X^{2}) \oeq{F.T} \sum_{k \in \N^{*}}k^{2} \frac{1}{\zeta(3)k^{3}} = \frac{1}{\zeta(3)} \sum_{k \in \N^{*}}\frac{1}{k} = +\infty
    $$
    donc $X \times X \notin L^{1}$.
  - Si $X \in L^{1}$ et $Y$ est une variable aléatoire bornée presque sûrement, alors il existe $M \in \R_{+}$ tel que $\abs{Y} \leq M$ $\P$-presque sûrement d’où $\abs{XY} \leq M \abs{X}$ presque sûrement or $M \abs{X} \in L^{1}$ donc $XY \in L^{1}$. En particulier~:
    %callout
      Pour $X \in L^{1}$ et $A$ un événement, on a $X\1_{A} \in L^{1}$.
    %
%

%prop
  1. Si $X$ et $Y$ sont deux variables aléatoires discrètes indépendantes de $L^{1}$, alors $XY \in L^{1}$ et $E(XY) = E(X) E(Y)$.
  2. Si $X_{1}, \ldots, X_{n}$ sont des variables aléatoires discrètes indépendantes de $L^{1}$, alors $(X_{1}X_{2} \cdots X_{n}) \in L^{1}$ et $E(X_{1} X_{2} \cdots X_{n}) = \prod_{i=1}^{n}E(X_{i})$.
%

%proof
  1. Soient $X \in L^{1}$ et $Y \in L^{1}$ indépendantes. $XY = f(X,Y)$ avec $f: \applic{\K^{2}}{\K}{(x,y)}{xy}$. D’après le _théorème de transfert_, on a
     $$
       XY \in L^{1} \iff \Big(xy\P \big((X,Y)=(x,y)\big)\Big)_{(x,y) \in (X,Y)(\Omega)} \in \ell^{1} \big((X,Y)(\Omega)\big)
     $$
     Dans $\bar{\R_{+}}$,
     $$
       \begin{align*}
         \sum_{(x,y) \in (X,Y)(\Omega)} \abs{xy}\P \big((X,Y)=(x,y)\big) &\ueq{X\indep Y}  \sum_{(x,y) \in X(\Omega)\times Y(\Omega)} \P(X=x) \P(Y=y) \\
         &= \sum_{x \in X(\Omega)}\abs{x}\P(X=x) \times \sum_{y \in Y(\Omega)}\abs{y} \P(Y=y) < +\infty
       \end{align*}
     $$
     donc $XY \in L^{1}$. De plus,
     $$
       \begin{align*}
         E(XY) &\oeq{T.T} \sum_{(x,y) \in (X,Y)(\Omega)} xy \P(X=x,Y=y) \\
         &= \sum_{x \in X(\Omega)}x \P(X=x) \times \sum_{y \in Y(\Omega)}y\P(Y=y) \\
         &= E(X)E(Y)
       \end{align*}
     $$
  2. Considérons la propriété $\Prop(n)$: «si $X_{1}, \ldots, X_{n}$ sont $n$ variables aléatoires discrètes indépendantes de $L^{1}(\Omega)$, alors $(X_{1}, \ldots, X_{n}) \in L^{1}(\Omega)$ et $E(X_{1}, \ldots, X_{n}) = \prod_{i=1}^{n}E(X_{i})$» définie pour $n \in \N^{*}$.
     - $\Prop(1)$ est vraie et $\Prop(2)$ aussi
     - Supposons $\P(n)$ vraie pour $n$ fixé dans $\N^{*}$. Soient $X_{1}, \ldots, X_{n+1}$ des variables aléatoires discrètes indépendantes de $L^{1}$. D’après le _lemme des coalitions_, $(X_{1} \cdots X_{n})$ et $X_{n+1}$ sont indépendantes et d’après $\Prop(n)$, $(X_{1} \cdots X_{n}) \in L^{1}$ donc d’après $\Prop(2)$, $(X_{1} \cdots X_{n}) \times X_{n+1} \in L^{1}$ et
       $$
         E \big((X_{1} \cdots X_{n}) \times X_{n+1}\big) = E(X_{1} \cdots X_{n})E(X_{n+1}) = \prod_{i=1}^{n}E(X_{i}) \times E_{X_{n+1}} = \prod_{i=1}^{n+1}E(X_{i})
       $$
       donc $\Prop(n+1)$ est vraie.
%

%custom Cas d’utilisatoin fréquent
  Si $(X_{1}, \ldots, X_{n})$ est une suite de variables aléatoires indépendantes identiquement distribuées de $L^{1}(\Omega)$, alors $X_{1} \times \cdots \times X_{n} \in L^{1}(\Omega)$ et $E(X_{1} \times  \cdots \times X_{n}) = E(X_{1})^{n}$
%

%prop Inégalité de Markov
  1. Si $X: (\Omega,\Tribu,\P) \longrightarrow \R_{+}$ est une variable aléatoire discrète positive (d’espérance finie)((Hypothèse non nécessaire.)), alors
     $$
       \forall a > 0,\P(X \geq a) \leq \frac{E(X)}{a}
     $$
  2. Si $X: (\Omega,\Tribu,\P) \longrightarrow \C$ est une variable aléatoire discrète de $L^{1}(\Omega)$, alors
     $$
       \forall a > 0,\P \big(\abs{X} \geq a\big) \leq \frac{E(\abs{X})}{a}
     $$
%

%proof
  1. On a ==$X \geq X\1_{(X \geq a)} \geq a \1_{(X \geq a)}$== d’où
     $$
       E(X) \geq E(a \1_{(X \geq a)}) = a\P(X \geq a)
     $$
  2. $X \in L^{1}(\Omega) \iff \abs{X}\in L^{1}(\Omega)$
%

%rem
  - Si $X \geq 0$ et $E(X)\ne 0$, alors
    $$
      \forall a > 0, \P(X \geq a E(X)) \leq \frac{E(X)}{aE(X)}
    $$
    Par exemple, $\P(X \geq 2 E(X)) \leq \frac{1}{2}$.
  - Soit $X: (\Omega,\Tribu,\P) \longrightarrow \R$ une variable aléatoire discrète.
    $$
      \forall a > 0, \P(\abs{X} \leq a) \leq e^{a^{2}} E(e^{-X^{2}})
    $$
    Soit $a > 0$.
    $$
      \P(\abs{X} \leq a) = \P(X^{2}\leq a^{2}) = \P(e^{-X^{2}} \geq e^{-a^{2}})
    $$
    or $0 \leq e^{-X^{2}} \leq 1$ donc $e^{-X^{2}} \geq 0$((Non nécessaire.)) et $e^{-X^{2}} \in L^{1}$ d’où
    $$
      \P(\abs{X} \leq a) \leq \frac{E(e^{-X^{2}})}{e^{-a^{2}}}
    $$
%

## Variance d’une variable aléatoire discrète _réelle_

Dans la suite, toutes les variables aléatoires discrètes sont réelles et définies, sauf avis contraire, sur un espace probabilisé $(\Omega,\Tribu,\P)$. On a donc
$$
  L^{1} = \big\{X: \Omega \longrightarrow \R \text{ v.a.d} \where E(\abs{X}) < +\infty\big\}
$$
l’ensemble des variable aléatoires d’ordre 1.

%def
  On pose
  $$
    L^{2}(\Omega) = \big\{X: \Omega \longrightarrow \R \text{ v.a.d} \where X^{2} \in L^{1}\big\}
  $$
  i.e.
  $$
    L^{2}(\Omega) = \big\{X: \Omega \longrightarrow \R \text{ v.a.d} \where E(X^{2}) < +\infty\big\}
  $$
  Pour $X \in L^{2}$, $E(X^{2})$ est appelé _moment d’ordre 2 de $X$_.
%

%eg
  - Si $X(\Omega)$ est une partie finie de $\R$, alors $X \in L^{2}$.
  - %callout
      Si $X$ est une variable aléatoire discrète réelle bornée $\P$-presque sûrement, alors $X \in L^{2}$.
    %
    En effet, il existe $M \in \R_{+}$ tel que $\abs{X} \leq M \ps$ donc $0 \leq X^{2} \leq M^{2} \ps$. Or $M^{2} = M^{2}\1_{\Omega} \in L^{1}$ donc $X^{2} \in L^{1}$.
%

%prop
  Soit $p \in \oo{0,1}$. Soit $n \in \N^{*}$. Soit $\lambda > 0$. Si $X \leadsto \B(p)$ ou si $X \leadsto \B(n,p)$ ou si $X \leadsto \mathscr{P}(\lambda)$ ou si $X\leadsto \mathscr{G}p$, alors $X \in L^{2}(\Omega)$.
%

%proof
  - si $X \leadsto \B(p)$ ou $X \leadsto \B(n,p)$, alors $X(\Omega)$ est fini.
  - si $X \leadsto \mathscr{P}(\lambda)$, alors on a dans $\bar{\R_{+}}$~:
    $$
      \begin{align*}
        E(X^{2}) &= \sum_{k \in \N}k^{2}\P(X=k) \\
        &= \sum_{k \in \stress{\N^{*}}} k^{2}e^{-\lambda} \frac{\lambda^{k}}{k!} \\
        &= \sum_{k \in \N^{*}} \big[k(k-1) + k\big] e^{-\lambda}\frac{\lambda^{k}}{k!} \\
        &= \sum_{k \in \ico{2,+\infty}} e^{-\lambda} \frac{\lambda^{k}}{(k-2)!} + \sum_{k \in \N^{*}} k e^{-\lambda}\frac{\lambda^{k}}{k!} \\
        &= e^{-\lambda}\lambda^{2} \sum_{k=0}^{+\infty}\frac{\lambda^{n}}{n!} + E(X) \\
        &= \lambda^{2} + \lambda < +\infty
      \end{align*}
    $$
  - si $X \leadsto \mathscr{G}(p)$, alors on a dans $\bar{\R_{+}}$~:
    $$
      \begin{align*}
        E(X^{2}) &= \sum_{k \in \N^{*}}k^{2}\P(X=k) \\
        &= \sum_{k \in \N^{*}}k^{2}p(1-p)^{k-1} \\
        &= \sum_{k \in \ico{2,+\infty}}k(k-1)p(1-p)^{k-1} + \sum_{k \in \N^{*}} kp(1-p)^{k-1} \\
        &= p(1-p)\sum_{k \in \ico{2,+\infty}}k(k-1)(1-p)^{k-2} + E(X) \\
        &= p(1-p) \eval{\dvN{}{x}{2} \left(\sum_{n=0}^{+\infty}x^{n}\right)}{x=1-p \in \oo{0,1}} + \frac{1}{p} \\
        &= p(1-p) \eval{\frac{2}{(1-x)^{3}}}{x=1-p} + \frac{1}{p} \\
        &= p(1-p)\frac{2}{p^{3}} + \frac{1}{p} = \frac{2}{p^{2}} - \frac{1}{p} < +\infty
      \end{align*}
    $$
%

%prop
  1. Si $X \in L^{2}(\Omega)$, alors $X \in L^{1}(\Omega)$.
  2. $L^{2}(\Omega)$ est un sous-espace vectoriel $L^{1}(\Omega)$.
  3. Si $(X,Y) \in L^{2} \times L^{2}$, alors $X \times Y \in L^{1}$ et
     $$
       \abs{E(XY)} \leq \sqrt{E(X^{2})}\sqrt{E(Y^{2})}
     $$
     i.e.
     $$
       \tag{$\star$} \left[E(XY)\right]^{2} \leq E(X^{2})E(Y^{2})
     $$
     (_inégalité de Cauchy-Schwarz_) et $(\star)$ est une égalité si et seulement si $(X,Y)$ est une famille liée de $L^{2}(\Omega)$ $\P$-presque sûrement.
%

%proof
  1. $\forall x \in \R, \abs{x} \leq \frac{1+x^{2}}{2}$ donc $\abs{X} \leq \frac{1+X^{2}}{2}$. Si $X \in L^{2}$, alors $(\1_{\Omega}, X^{2}) \in L^{1}$. Comme $L^{1}$ est un sous-espace de $L^{0}$, on a $\frac{1+X^{2}}{2} \in L^{1}$ donc $X \in L^{1}$.
  2. $L^{2} \subset L^{1}$ et $\1_{\emptyset} \in L^{2}$. Soient $(X,Y) \in L^{2} \times L^{2}$ et $\lambda \in \R$. $(\lambda X)^{2} = \lambda^{2} \cdot X^{2} \in L^{1}$ car $X^{2} \in L^{1}$ et $L^{1}$ est un sous-espace vectoriel de $L^{0}$ donc $\lambda X \in L^{2}$. $(X+Y)^{2} = X^{2} + 2XY + Y^{2} \leq X^{2} + (X^{2} + Y^{2}) + Y^{2}$ donc $0 \leq (X+Y)^{2} \leq 2(X^{2} + Y^{2})$ or $(X^{2},Y^{2}) \in L^{1}$ et $L^{1}$ est un sous-espace vectoriel de $L^{0}$ donc $2(X^{2} + Y^{2}) \in L^{1}$ donc $(X+Y)^{2} \in L^{1}$ donc $X+Y \in L^{2}$.
  3. $\abs{XY} \leq \frac{X^{2} + Y^{2}}{2}$ et $\frac{X^{2} + Y^{2}}{2} \in L^{1}$ donc $XY \in L^{1}$. On a $\forall t \in \R, E \big((tX+Y)^{2}\big) \geq 0$ donc
     $$
       \forall t \in \R, t^{2}E(X^{2}) + 2tE(XY) + E(Y^{2}) \geq 0
     $$
     - Si $E(X^{2}) = 0$, alors $X^{2}=0 \ps$ donc $X = 0 \ps$ donc $XY = 0 \ps$ donc $E(XY) = \sqrt{E(X^{2})} \times \sqrt{E(Y^{2})}$ donc $(\star)$ est une égalité.
     - Supposons $E(X^{2}) > 0$. Alors $\Delta' = E(XY)^{2} - 2 E(X^{2})E(Y^{2}) \leq 0$((Discriminant réduit: pour les polynômes de la forme $ax^{2} + 2bx + c = 0$, il vaut $\Delta' = b^{2} - ac$ et les racines sont $x = \frac{-b' \pm \sqrt{\Delta'}}{a}$)) donc $E(XY)^{2} \leq E(X^{2})E(Y^{2})$. Si l’inégalité ci-dessus est une égalité, alors il existe $t_{0} \in \R$ tel que $E \big((t_{0} X + Y)^{2}\big) = 0$ donc $t_{0}X + Y = 0 \ps$.
%

%def
  Soit $X \in L^{2}$. Alors $X - E(X) \in L^{2}$ donc $(X - E(X))^{2} \in L^{1}$. On appelle _variance de $X$_ le réel positif((C’est la moyenne des carrés des écarts à la moyenne.))
  $$
    V(X) = E \Big(\big(X - E(X)\big)^{2}\Big)
  $$
  On appelle _écart-type de $X$_ le réel positif
  $$
    \sigma(X) = \sqrt{V(X)}
  $$
  On dit que $X$ est _réduite_ si $\sigma(X) = 1$.
%

%prop
  Soit $X: (\Omega,\Tribu,\P) \longrightarrow \R$ un élément de $L^{2}$ «de variance finie».
  1. $V(X) \geq 0$ et $V(X) = 0 \iff X$ constante $\P$-presque sûrement.
  2. $\forall (a,b)\in \R^{2}, V(aX+b) = a^{2}V(X)$
  3. $V(X) = E(X^{2}) - E(X)^{2}$ (_formule de Koenig-Huyghens_)
%

%proof
  1. $V(X) = E \Big(\big(X-E(X)\big)^{2}\Big) \geq 0$ donc
     $$
       V(X) = 0 \iff X-E(X) = 0 \ps \implies X \text{ constante} \ps
     $$
     Réciproquement, si $X = \lambda \ps$ avec $\lambda \in \R$, alors $E(X) = \lambda = X \ps$ donc $V(X) = E \Big(\big(X-E(X)\big)^{2}\Big) = 0$.
  2. On a
     $$
       \begin{align*}
         V(aX+b) &= E \Big(\big(aX+b - E(aX+b)\big)^{2}\Big) \\
         &= E \Big(\big(aX + b - aE(X) - b\big)^{2}\Big) \\
         &= E \Big(a^{2} \big(X-E(X)\big)^{2}\Big) \\
         &= a^{2}V(X)
       \end{align*}
     $$
  3. On a
     $$
       V(X) = E \Big(\big(X - E(X)\big)^{2}\Big) = E \big(X^{2} - 2E(X) \cdot X + E(X)^{2}\big)
     $$
     or $(X^{2}, X, \1_{\Omega}) \in (L^{1})^{3}$ donc par linéarité, on a
     $$
       V(X) = E(X^{2}) - 2E(X)E(X) + E(X)^{2} = E(X^{2}) - E(X)^{2}
     $$
%

%rem
  Si $X \in L^{2}(\Omega)$ et $Y \in L^{0}(\Omega')$ suivent la même loi, alors $Y \in L^{2}(\Omega')$ et $E(Y) = E(Y)$ et $V(Y) = V(X)$.
  En effet~:
  $$
    \begin{align*}
      E(Y^{2}) \underset{\bar{\R_{+}}}{\oeq{F.T}} &= \sum_{y \in Y(\Omega')}y^{2}\P'(Y=y) \\
        &= \sum_{y \in Y(\Omega')\cup X(\Omega)} y^{2}\P'(Y=y) \\
        &= \sum_{y \in Y(\Omega') \cup X(\Omega)} y^{2}\P(X=y) \\
        &= \sum_{y \in X(\Omega)}y^{2}\P(X=y) \\
        &= E(X^{2}) < +\infty
    \end{align*}
  $$
%

%prop
  Pour $p \in \oo{0,1}$, $n \in \N^{*}$ et $\lambda > 0$, on a((==TODO: Implémenter les tableaux==))

  | $X \leadsto$ | $E(X)$ | $V(X)$ |
  | --- |
  | $\B(p)$ | $p$ | $pq$ |
  | $\B(n,p)$ | $np$ | $npq$ |
  | $\mathscr{P}(\lambda)$ | $\lambda$ | $\lambda$ |
  | $\mathscr{G}(p)$ | $\frac{1}{p}$ | $\frac{q}{p^{2}}$ |

  ~

  ```
    ┌──────────┬──────┬────────┐
    │  X ↝     │ E(X) │  V(X)  │
    ├──────────┼──────┼────────┤
    │  B(p)    │  p   │   pq   │
    ├──────────┼──────┼────────┤
    │  B(n,p)  │  np  │  npq   │
    ├──────────┼──────┼────────┤
    │  P(λ)    │  λ   │   λ    │
    ├──────────┼──────┼────────┤
    │  G(p)    │ 1/p  │  q/p²  │
    └──────────┴──────┴────────┘
  ```
%

%proof
  - Si $X \leadsto \B(p)$, alors $X \sim \1_{(X=1)}$ donc
    $$
      \begin{align*}
        V(X) &= V \left(\1_{(X=1)}\right) \\
        &= E \left(\1_{(X=1)}^{2}\right) - E(\1_{(X=1)})^{2}\\
        &= E(\1_{(X=1)}) - E(\1_{(X=1)})^{2} \\
        &= \P(X=1) - \P(X=1)^{1} \\
        &= p - p^{2} = p(1-p) = pq
      \end{align*}
    $$
  - Si $X \leadsto \B(n,p)$, alors $X \in L^{2}$ et $V(X) = E(X^{2})  E(X)^{2}$.
    $$
      \begin{align*}
        E(X^{2}) &= \sum_{k=0}^{n}k^{2}\P(X=k) \\
        &= \sum_{k=\stress 1}^{+\infty} k^{2} \binom{n}{k}p^{k}q^{n-k} \\
        &= \sum_{k=1}^{n}k^{2} \frac{n}{k} \binom{n-1}{k-1}p^{k}q^{n-k} \\
        &= np \sum_{k=1}^{n}k \binom{n-1}{k-1}p^{k-1} q^{n-k} \\
        &= np \sum_{k=0}^{n-1}(k-1) \binom{n-1}{k} p^{k}q^{n-1-k} \\
        &= np \left(\underbrace{\sum_{k=0}^{n-1}k \binom{n-1}{k}p^{k}q^{n-1-k}}_{E(Y) \text{ si } Y \leadsto \B(n-1,p)} + (p+q)^{n-1}\right)
      \end{align*}
    $$
    d’où $E(X^{2}) = np \big((n-1)p + 1\big)$ d’où
    $$
      V(X) = np \big((n-1)p + 1\big) - (np)^{2} = np(1-p) = npq
    $$
  - Si $X \leadsto \mathcal{P}(\lambda)$, alors
    $$
      V(X) = E(X^{2}) - E(X)^{2} = \lambda^{2} + \lambda - \lambda^{2} = \lambda
    $$
  - Si $X \leadsto \mathscr{G}(p)$, alors
    $$
      V(X) = E(X^{2}) - E(X)^{2} = \frac{2}{p^{2}} - \frac{1}{p} - \frac{1}{p^{2}} = \frac{1}{p^{2}} - \frac{1}{p} = \frac{1-p}{p^{2}} = \frac{q}{p^{2}}
    $$
%

%rem
  Si $X \in L^{2}(\Omega)$ et si $V(X) > 0$, alors
  $$
    Y = \frac{X - E(X)}{\sigma(X)}
  $$
  est _centrée réduite_.
  En effet, $ey \in el^{2}(\Omega)$ et
  - $E(Y) = \frac{1}{\sigma(X)}(E(X) - E(X)) = 0$
  - $V(Y) = \frac{1}{\sigma(X)^{2}}V(X) = 1$
%

%prop
  1. Soit $(X,Y)\in L^{2} \times L^{2}$. Alors $\big(X-E(X), Y - E(Y)\big)\in L^{1}$ donc $\big(X-E(X)\big)(Y-E(Y)) \in L^{1}$. On pose
     $$
       \cov(X,Y) = E \Big(\big(X-E(X)\big)(Y-E(Y))\Big)
     $$
     et on a
     - $\cov(X,Y) = E(XY) - E(X)E(Y)$ (formule de _Koenig-Huyghens_)
     - $X$ et $Y$ sont dites _décorrélées_ si $\cov(X,Y) = 0$
     - Si $X$ et $Y$ sont indépendantes, alors elles sont décorrelées.
  2. L’application
     $$
       \cov :\applic{L^{2} \times L^{2}}{\R}{(X,Y)}{\cov(X,Y)}
     $$
     est une forme bilinéaire symétrique positive. On a
     - $X \in L^{2}, \cov(X,X) = V(X) = 0$
     - $\forall (X,Y) \in (L^{2})^{2}, \cov(X,Y) = \cov(Y,X)$
     - $\forall (X,Y,Z) \in (L^{2})^{3}, \forall (a,b) \in \R^{2}, \cov(aX + bY,Z) = a\cov (X,Z) + b\cov(Y,Z)$
     - $\forall a \in \R, \forall X \in L^{2}, \cov(X,a) = 0$
     - $\forall (a,b,c,d) \in \R^{4}, \forall (X,Y) \in (L^{2})^{2}, \cov(aX+b,cY+d) = ac\cov(X,Y)$
%

%proof
  1. - $\cov(X,Y) = E \big(XY - E(X)Y - E(Y)X + E(X)E(Y)\big)$ or $(XY,Y,X,\1_{\Omega}) \in (L^{1})^{4}$ donc
       $$
         \cov(X,Y) = E(XY) - E(X)E(Y) - E(Y)E(X) + E(X)E(Y) = E(XY) - E(X)E(Y)
       $$
     - Si $(X,Y)\in (L^{2})^{2}$, alors $(X,Y) \in (L^{1})^{2}$ et $X \indep Y$ donc $E(XY) = E(X)E(Y)$ donc $\cov (X,Y) = 0$
  2. - On a
       $$
         \begin{align*}
           \cov(aX+bY,Z) &= E \big((aX + bY)Z\big) - E(aX + bY)E(Z) \\
           &= (aE(XY) + bE(YZ)) - aE(X)E(Z) - bE(Y)E(Z) \\
           &= a\cov(X,Z) + b\cov (Y,Z)
         \end{align*}
       $$
     - $\cov(X,a) = E(aX) - E(X)\times a = aE(X) - aE(X) = 0$
     - $\cov(aX + b,cY + d) = ac\cov(X,Y) + 0 = ac\cov(X,Y)$
%

%prop
  Soit $n \in \N^{*}$.
  - Si $(X_{1}, \ldots, X_{n}) \in (L^{2})^{n}$, alors
    $$
      V \left(\sum_{i=1}^{n}X_{i}\right) = \sum_{i=1}^{n}V(X_{i}) + 2 \sum_{1 \leq i < j \leq n} \cov(X_{i},X_{j})
    $$
  - Si $X_{1}, \ldots, X_{n}$ sont des variables aléatoires discrètes réelles de $L^{2}$ **décorrélées** deux à deux (voire indépendantes deux à deux, voire indépendantes), alors
    $$
      V \left(\sum_{i=1}^{n}X_{i}\right) = \sum_{i=1}^{n}V \left(X_{i}\right)
    $$
%

%proof
  $$
    \begin{align*}
      V \left(\sum_{i=1}^{n}X_{i}\right) &= \cov \left(\sum_{i=1}^{n}X_{i}, \sum_{j=1}^{n}X_{j}\right) \\
      &= \sum_{i=1}^{n}\cov(X_{i}, X_{j}) + \sum_{\substack{(i,j) \in \icc{1,n}^{2}\\ i\ne j}}\cov(X_{i}, X_{j})
    \end{align*}
  $$
%

%def Schéma de Bernoulli
  Soit $n \in \N^{*}$. $\big(\{0,1\}^{n}, \Part(\{0,1\}^{n})\big)$ est un espace probabilisable (fini). Considérons la famille
  $$
    \Big(p^{\varepsilon_{1} + \cdots + \varepsilon_{n}} (1-p)^{(1-\varepsilon_{1}) + \cdots + (1-\varepsilon_{n})}\Big)_{(\varepsilon_{1}, \ldots, \varepsilon_{n}) \in \{0,1\}^{n}}
  $$
  C’est une distribution de probabilité sur $\{0,1\}^{n}$, une famille de réels positifs, et
  $$
    \sum_{(\varepsilon_{1}, \ldots, \varepsilon_{n}) \in \{0,1\}^{n}} p^{\varepsilon_{1} + \cdots + \varepsilon_{n}}(1-p)^{(1-\varepsilon_{1}) + \cdots + (1-\varepsilon_{n})} = \left(\sum_{\varepsilon_{1} = 0}^{1}p^{\varepsilon_{1}}(1-p)^{1-\varepsilon_{1}}\right) \times  \cdots \times \left(\sum_{\varepsilon_{n} = 0}^{1} p^{\varepsilon_{n}} (1-p)^{1-\varepsilon_{n}}\right) = (1-p+p)^{n} = 1
  $$
  Notons $\P$ la probabilité sur $\Big(\{0,1\}^{n}, \Part(\{0,1\}^{n})\Big)$
  Notons pour $i \in \icc{1,n}$
  $$
    X_{i}: \applic{\{0,1\}^{n}}{\{0,1\}}{(\varepsilon_{i})_{1 \leq i \leq n}}{\varepsilon_{i}}
  $$
  C’est une variable aléatoire discrète. Soit $\varepsilon \in \{0,1\}$. On a
  $$
    \begin{align*}
      \P(X_{i} = \varepsilon) &= \P \big(\{0,1\}^{i-1} \times \{\varepsilon\} \times \{0,1\}^{n-i}\big) \\
        &= \sum_{\substack{(\varepsilon_{1}, \ldots, \varepsilon_{n}) \in \{0,1\}^{n}\\ \varepsilon_{i} = \varepsilon}}p^{\varepsilon_{1} + \cdots + \varepsilon_{n}}(1-p)^{(1-\varepsilon_{1}) \cdots (1-\varepsilon_{n})} \\
        &= \prod_{\substack{k=1\\k\ne 1}}^{n}\left(\sum_{\varepsilon_{k} = 0}^{1}p^{\varepsilon_{k}}(1-p)^{1-\varepsilon_{k}}\right) \times p^{\varepsilon} (1-p)^{1-\varepsilon} \\
        &= p^{\varepsilon}(1-p)^{1-\varepsilon} \\
        &= \begin{cases}p \if \varepsilon=1 \\ 1-p \if \varepsilon=0\end{cases}
    \end{align*}
  $$
  donc $\forall i \in \icc{1,n}, X_{i}\leadsto \B(p)$. Soit $(\varepsilon_{1}, \ldots, \varepsilon_{n}) \in \{0,1\}^{n}$.
  $$
    \begin{align*}
      \P(X_{1} = \varepsilon_{1} , \ldots, X_{n} = \varepsilon_{n}) &= \P \big(\{(\varepsilon_{1}, \ldots, \varepsilon_{n})\}\big) \\
      &= p^{\varepsilon_{1} + \cdots + \varepsilon_{n}} (1-p)^{(1-\varepsilon_{i}) + \cdots + (1-\varepsilon_{n})} \\
      &= \prod_{i=1}^{n}p^{\varepsilon_{i}}(1-p)^{1-\varepsilon_{i}} \\
      &= \prod_{i=1}^{n}\P(X_{i} = \varepsilon_{i})
    \end{align*}
  $$
  donc $(X_{1}, \ldots, X_{n})$ est une suite de variables aléatoires (aka. $n$-échantillon) indépendantes identiquement distribuées.
%

%prop Inégalité de Bienaymé-Tchebychev
  Si $X: \Omega \longrightarrow \R$ est une variable aléatoire réelle discrète de $L^{2}(\Omega)$, alors
  $$
    \forall  \varepsilon> 0, \P(\abs{X-E(X)} \geq \varepsilon) \leq \frac{V(X)}{\varepsilon^{2}}
  $$
%

%proof
  $\big(\abs{X-E(X)} \geq \varepsilon\big) = \big((X-E(X))^{2} \geq \varepsilon^{2}\big)$ or $X \in L^{2}$ donc $(X-E(X))^{2} \in L^{1}$ d’où d’après _l’inégalité de Markov_, on a
  $$
    \P \big(\abs{X-E(X)} \geq \varepsilon\big) \leq \frac{E \big((E-E(X))^{2}\big)}{\varepsilon^{2}} = \frac{V(X)}{\varepsilon^{2}}
  $$
%

%cor Loi faible des grands nombres
  Si $(X_{n})_{n \in \N^{*}}$ est une suite de variables aléatoires **indépendantes identiquement distribuées** de $L^{2}(\Omega)$, alors si on pose $S_{n} = \sum_{k=1}^{n}X_{k}$, on a
  $$
    \forall \varepsilon > 0, \P \left(\abs{\underbrace{\frac{S_{n}}{n}}_{\text{moy. empirique}} - \underbrace{E(X_{1})}_{\text{moy. théorique}}} \geq \varepsilon\right) \arrowlim{n\to+\infty}0
  $$
%

%proof
  $\frac{S_{n}}{n} = \frac{1}{n}\sum_{k=1}^{n}X_{k} \in L^{2}$ car $L^{2}$ est un sous-espace vectoriel de $L^{1}$.
  $$
    E \left(\frac{S_{n}}{n}\right) = \frac{1}{n} \sum_{k=1}^{n}E(V_{k}) = \frac{1}{n}\sum_{k=1}^{n}E(X_{1}) = E(X_{1})
  $$
  et par indépendance de $(X_{1}, \ldots, X_{n})$,
  $$
    V \left(\frac{S_{n}}{n}\right) = \frac{1}{n^{2}} V(S_{n}) = \frac{1}{n^{2}} \sum_{k=1}^{n}V(X_{k}) = \frac{1}{n^{2}} nV(X_{1}) = \frac{V(X_{1})}{n}
  $$
  D’après l’inégalité de _Bienaymé-Tchébychev_, on a
  $$
    \forall \varepsilon > 0, \P \left(\abs{\frac{S_{n}}{n} - E(X_{1})} \geq \varepsilon\right) \leq \frac{V(X_{1})}{n \varepsilon^{2}} \arrowlim{n\to+\infty} 0
  $$
%

%rem
  Interprétation: on a
  $$
    \forall \varepsilon > 0, \P \left(\abs{\frac{S_{n}}{n} - E(X_{1})} < \varepsilon\right) \arrowlim{n\to+\infty}1
  $$
  L’écart entre la moyenne empirique et la moyenne théorique peut être rendu aussi petit que l’on veut avec une probabilité aussi grande que l’on veut.
%
