# Dérivabilité et différentiabilité

## Arc paramétrés dérivables

Dans cette partie, $I$ est un intervalle réel d’intérieur non vide, $(E,\norm{\cdot})$ est un $\K$-espace vectoriel normé de dimension finie dont une base est $(e_{1}, \ldots, e_{n})$.

%offprog
  Soit $x \in E$. Posons $x = \sum_{i=1}^{n}x_{i}e_{i}$ avec $(x_{1}, \ldots, x_{n}) \in \K^{n}$. On note, pour $i \in \icc{1,n}$,
  $$
    e_{i}^{*}: \applic{E}{\K}{x}{x_{i}}
  $$
  la i-ème forme linéaire coordonnée dans la base $(e_{1}, \ldots, e_{n})$. $(e_{1}^{*}, \ldots, e_{n}^{*})$ est alors une base de $\L(E,\K)$ appelée la base duale de $(e_{1}, \ldots, e_{n})$.
%

%recall
  Pour $E$ et $F$ de dimension finie, $\dim \L(E,F) = \dim E \times \dim F$ donc $\dim \L(E,\K) = \dim E = n$.
  $$
    \forall i \in \icc{1,n}, e_{i}^{*} \in \L(E,\K)
  $$
  Soit $(\lambda_{1}, \ldots, \lambda_{n}) \in \K^{n}$ avec $\lambda_{1} e_{1}^{*} + \cdots + \lambda_{n}e_{n}^{*} = 0_{E^{*}}$. On a alors pour tout $i \in \icc{1,n}$,
  $$
    (\lambda_{1}e_{1}^{*} + \cdots + \lambda_{n}e_{n}^{*})(e_{i}) = 0
  $$
  i.e. $\lambda_{i} = 0$. Ainsi, la famille $(e_{1}^{*}, \ldots, e_{n}^{*})$ est une famille libre de $E^{*}$ de longueur $n = \dim E^{*}$; c’est donc une base.
%

%def
  - Une application $f: \applic{I}{E}{t}{f(t)}$ est appelée *fonction vectorielle* ou *arc paramétré*.
  - $f(I)$ est appelé *support* de l’arc.
  - Si $f_{i} = e_{i}^{*} \circ f$, i.e.
    $$
      \forall t \in I, f(t) = \sum_{i=1}^{n}f_{i}(t)e_{i}
    $$
    alors
    $$
      \begin{cases}x_{1}&= f_{1}(t) \\ &\cdots\\x_{n}&= f_{n}(t)\end{cases}, t \in I
    $$
    est appelée *représentation paramétrique de l’arc*.
%

%eg
  L’application $f: \applic{\R}{\R^{3}}{t}{(\cos t, \sin t,t)}$ paramètre une spirale :
  ==DESSIN==
%

%def
  Soit $f: I \longrightarrow E$ un arc paramétré. Soit $t_{0} \in I$. On dit que $f$ est dérivable en $t_{0}$ si l’une des conditions équivalentes suivantes est vériiée :
  1. $t \longmapsto \frac{f(t)-f(t_{0})}{t-t_{0}} \in E$ (définie sur $I \setminus \{t_{0}\}$) admet une limite dans $E$ en $t_{0}$ (on note cette limite $f'(t_{0})$).
  2. $f$ possède un développement limité à l’ordre 1 en $t_{0}$ : il existe $u \in E$ et $\varepsilon \in \Func(I,E)$ tels que
     $$
       \forall t \in I, f(t) = f(t_{0}) + (t-t_{0})u + (t-t_{0})\varepsilon(t)
     $$
     avec $\varepsilon(t) \arrowlim{t\to t_{0}}0_{E}$.
  3. (si $t_{0}$ est dans l’intérieur de $I$)
     $$
       \lim_{t\to t_{0}^{+}}\frac{f(t)-f(t_{0})}{t-t_{0}} \quad\text{et}\quad \lim_{t\to t_{0}^{-}} \frac{f(t)-f(t_{0})}{t-t_{0}}
     $$
     existent dans $E$ et sont égales.
%

%proof
  - Supposons le premier point. Posons
    $$
      \varepsilon(t) = \begin{cases}\frac{f(t)-f(t_{0})}{t-t_{0}}-f'(t_{0}) \if t\ne t_{0}\\ 0\if t=t_{0}\end{cases}
    $$
    On a $\lim_{b\to t_{0}} \varepsilon(t) = 0_{E}$, d’où
    $$
      \forall t \in I, f(t) = f(t_{0}) + (t-t_{0})f'(t_{0})+(t-t_{0})\varepsilon(t)
    $$
  - Supposons le deuxième point. On a alors
    $$
      \forall t \in I \setminus \{t_{0}\}, \frac{f(t)-f(t_{0})}{t-t_{0}} = u+\varepsilon(t)\arrowlim{t\to t_{0}} u \in E
    $$
%

%prop
  Soit $f: I\longrightarrow E$ un arc paramétré. Soit $t_{0} \in I$.
  1. Si $f$ est dérivable en $t_{0}$, alors $f$ est continue en $t_{0}$
  2. Pour $f= \sum_{i=1}^{n}f_{i}e_{i}$ avec $f_{i} = e_{i}^{*}\circ f \in \Func(I,\K)$ pour tout $i \in \icc{1,n}$, on a
     $$
       f \text{ dérivable en }t_{0} \iff \text{chaque } f_{i} \text{ est dérivable en }t_{0}
     $$
     Si tel est le cas, alors
     $$
       f'(t_{0}) = \sum_{i=1}^{n}f_{i}'(t_{0})e_{i}
     $$
  3. $f$ est constante sur $I$ si et seulement si
     ~ $f$ est continue sur $I$  ~ $f$ est dérivable sur $\ring I$  ~ $f'=0$ sur $\ring I$
  4. Si $g: I \longrightarrow E$ est un second arc et si $f$ et $g$ sont dérivables en $t_{0}$, alors $\lambda f + \mu g$ est dérivable en $t_{0}$ pour tous $(\lambda, \mu) \in \K^{2}$, et
     $$
       (\lambda f+\mu g)'(t_{0}) = \lambda f'(t_{0}) + \mu g'(t_{0})
     $$
  5. Si $L: E\longrightarrow F$ est une application linéaire (avec $F$ un $\K$-espace vectoriel normé de dimension finie) et si $f$ est dérivable en $t_{0}$, alors $L\circ f: I \longrightarrow F$ est dérivable en $t_{0}$ et
     $$
       (L\circ f)'(t_{0}) = L(f'(t_{0}))
     $$
     On note parfois $L(f) = L\circ f$.
  6. Si $M: E_{1} \times  \cdots \times E_{p} \longrightarrow F$ est une application $p$-linéaire avec $E_{1}, \ldots, E_{p}$ et $F$ des $\K$-espaces vectoriels normés de dimension finie, et si $g_{1}: I\longrightarrow E_{1}, \ldots, g_{p}: I\longrightarrow E_{p}$ sont des arcs dérivables en $t_{0}$, alors
     $$
       « M(g_1, \ldots, g_{p}) » : \applic{I}{F}{t}{M(g_{1}(t), \ldots, g_{p}(t))}
     $$
     est dérivable en $t_{0}$ et
     $$
       (M(g_{1}, \ldots, g_{p}))'(t_{0}) = \sum_{i=1}^{p}M(g_{1}(t_{0}), \ldots, g_{i-1}(t_{0}), g_{i}'(t_{0}), g_{i+1}(t_{0}), \ldots, g_{p}(t_{0}))
     $$
%

%proof
  1. On a
     $$
       \forall t \in I, f(t) = f(t_{0}) + (t-t_{0})f'(t_{0}) + (t-t_{0})\varepsilon(t)
     $$
     avec $\varepsilon(t) \arrowlim{t\to t_{0}}0_{E}$, d’où $f(t) \arrowlim{t\to t_{0}}f(t_{0})$.
  2. On a
     $$
       \forall t \in I \setminus \{t_{0}\}, \underbrace{\frac{\vec{f}(t) - \vec{f}(t_{0})}{t-t_{0}}}_{\in E} = \sum_{i=1}^{n}\frac{f_{i}(t) - f_{i}(t_{0})}{t-t_{0}}\vec{e_{i}}
     $$
     et pour tout $i \in \icc{1,n}$, $t \longmapsto \frac{f(t)-f(t_{0})}{t-t_{0}}$ admet une limite dans $E$ en $t_{0}$ si et seulement si $t \longmapsto \frac{f_{i}(t)-f_{i}(t_{0})}{t-t_{0}}$ admet une limite dans $\K$ en $t_{0}$. En cas de dérivabilité en $t_{0}$,
     $$
       f'(t_{0}) = \lim_{t\to t_{0}}\sum_{i=1}^{n}\frac{f_{i}(t)-f_{i}(t_{0})}{t-t_{0}}e_{i} = \sum_{i=1}^{n}f_{i}'(t_{0})e_{i}
     $$
  3. $f$ est constante sur $I$ si et seulement si chaque $f_{i}: I \longrightarrow\K$ est constante sur $I$, ce qui est le cas si et seulement si pour tout $i \in \icc{1,n}$,
     ~ $f_{i}$ est continue sur $I$  ~ $f_{i}$ est dérivable sur $\ring I$  ~ $f_{i}' = 0$ sur $I$
     ce qui est le cas si et seulement si
     ~ $f$ est continue sur $I$  ~ $f$ est dérivable sur $I$  ~ $f'=0$ sur $I$
  4. $f(t) = f(t_{0}) + (t-t_{0})f'(t_{0}) + (t-t_{0})\varepsilon_{f}(t)$ et $g(t) = g(t_{0}) + (t-t_{0})g'(t_{0}) + (t-t_{0})\varepsilon_{g}(t)$ avec
     ~ $\varepsilon_{f}(t) \arrowlim{t\to t_{0}}0_{E}$  ~ $\varepsilon_{g}(t) \arrowlim{t\to t_{0}}0_{E}$
     d’où
     $$
       (\lambda f+\mu g)(t) = \lambda f(t) + \mu g(t) = \lambda f(t_{0}) + \mu g(t_{0}) + (t-t_{0})(\lambda f'(t_{0})+\mu g'(t_{0})) + (t-t_{0})\varepsilon (t)
     $$
     avec $\varepsilon(t) = \lambda \varepsilon_{f}(t) + \mu \varepsilon_{g}(t) \arrowlim{t\to t_{0}}0_{E}$. Ainsi, $\lambda f+\mu g$ est dérivable est $t_{0}$.
  5. $\frac{L(f(t)) - L(f(t_{0}))}{t-t_{0}} = L \left(\frac{f(t) - f(t_{0})}{t-t_{0}}\right)$ car $el$ est linéaire. Si $f$ est dérivable en $t_{0}$, alors
     $$
       \frac{f(t) - f(t_{0})}{t-t_{0}} \arrowlim{t\to t_{0}}f'(t_{0})
     $$
     et par continuité de $L$ ($L$ est linéaire et de dimension finie), on a
     $$
       \frac{(L\circ f)(t) - (L\circ f)(t_{0})}{t-t_{0}} \arrowlim{t\to t_{0}} L(f'(t_{0}))
     $$
     Donc $L\circ f$ est dérivable en $t_{0}$ et $(L\circ f)'(t_{0}) = L(f'(t_{0}))$.
  6. On a
     $$
       \begin{align*}
         \frac{M(g_{1}(t), \ldots, g_{p}(t) - M(g_{1}(t_{0}), \ldots, g_{p}(t_{0})))}{t-t_{0}} &= \frac{1}{t-t_{0}}\sum_{i=1}^{p}M(g_{1}(t_{0}), \ldots, g_{i-1}(t_{0}), g_{i}(t)-g_{i}(t_{0}), g_{i+1}(t), \ldots, g_{p}(t))\\
         &=\sum_{i=1}^{p} M(g_{1}(t_{0}), \ldots, g_{i-1}(t_{0}), \frac{g_{i}(t) - g_{i}(t_{0})}{t-t_{0}}, g_{i+1}(t), \ldots, g_{p}(t))
       \end{align*}
     $$
     or
     ~ $\frac{g_{i}(t) - g_{i}(t_{0})}{t-t_{0}}\arrowlim{t\to t_{0}}g_{i}'(t_{0})$  ~ $g_{i+1}(t) \arrowlim{t\to t_{0}}g_{i+1}(t_{0})$  ~ …  ~ $g_{p}(t) \arrowlim{t\to t_{0}}g_{p}(t_{0})$  ~ $M$ est continue (forme $p$-linéaire sur un produit d’espaces vectoriels de dimension finie)
     d’où la dérivabilité de $M(g_{1}, \ldots, g_{p})$ et la formule annoncée.
%

%eg
  - Soit $A: \applic{I}{\M_{n}(\K)}{t}{A(t)}$ un arc dérivable. Soit $p \in \N^{*}$. Posons $f: \applic{I}{\M_{n}(\K)}{t}{(A(t))^{p}}$.
    $$
      M: \applic{\M_{n}(\K)^{p}}{\M_{n}(\K)}{(A_{1}, \ldots, A_{p})}{A_{1} \times  \cdots \times  A_{p}}
    $$
    est $p$-linéaire et $f = M(A, \ldots, A)$ donc $f$ est dérivable sur $I$ et
    $$
      \forall t \in I, f'(t) = A'(t)(A(t))^{p-1} + A(t) A'(t)A(t)^{p-2} + \cdots + A(t)^{p-1}A'(t)
    $$
    Si de plus $\forall t \in I, [A(t), A'(t)] = 0_{n}$ (ce qui est très rare), alors
    $$
      \forall t \in I, f'(t) = p A'(t) A(t)^{p-1}
    $$
  - Si $\lambda : I \longrightarrow \K$ et $f: I \longrightarrow E$ sont dérivables en $t_{0} \in I$, alors $\lambda f:I \longrightarrow E$ est dérivable en $t_{0}$ et
    $$
      (\lambda f)'(t_{0}) = \lambda' (t_{0})f(t_{0}) + \lambda(t_{0})f'(t_{0})
    $$
    car $\lambda f = B(\lambda, f)$ avec $B: \applic{\K \times  E}{E}{(\lambda u)}{\lambda \cdot u}$ est bilinéaire.
  - Si $E$ est un espace vectoriel euclidien de produit scalaire $\scalar{\cdot}{\cdot}$ (et donc muni de la norme euclidienne associée), et si $f: I \longrightarrow E$ et $g: I \longrightarrow E$ sont deux arcs dérivables en $t_{0}$, alors $\scalar{f}{g} : t \longmapsto \scalar{f(t)}{g(t)}$ est dérivable en $t_{0}$ et
    $$
      \scalar{f}{g}'(t_{0}) = \scalar{f'(t_{0})}{g(t_{0})} +\scalar{f(t_{0})}{g'(t_{0})}
    $$
  - Si $f_{1}: I \longrightarrow E, \ldots, f_{n}: I \longrightarrow E$ sont $n$ arcs dérivables en $t_{0} \in I$ à valeurs dans $E$ de dimension $n$ muni d’une base $\B$, alors
    $$
      t \longmapsto \det_{\B} (f_{1}(t), \ldots, f_{n}(t))
    $$
    est dérivable en $t_{0}$ et
    $$
      \left(\det_{\B}(f_{1}, \ldots, f_{n})\right)'(t_{0}) = \sum_{i=1}^{n}\det_{\B}(f_{1}(t), \ldots, f_{i}'(t_{0}), \ldots, f_{n}(t_{0}))
    $$
    car $\det_{\B}$ est n-linéaire.

    %eg
      Soit $(a_{1}, \ldots, a_{n})\in \R^{n}$. Soit $x \in \R$. Calculons
      $$
        \vmtx{a_{1}+x & x && (x)\\ & a_{2}+x &&\\ & \vdots & \ddots & \\ (x) & x & & a_n + x} = \Delta(x)
      $$
      On a pour $C_{i} \leftarrow C_{i} - C_{n}$
      $$
        \Delta(x) = \vmtx{a_{1} & 0 & && &x\\0 & a_{2} & &&& \vdots\\\vdots&0 & \ddots && &\vdots\\\vdots & \vdots & & \ddots && \vdots\\ 0 & 0 && & a_{n-1} & x\\ -a_{n} & -a_{n} & && -a_{n} & a_{n}+x}
      $$
      En développant selon $C_{n}$, on montre que $\Delta$ est affine. On a $\forall x \in \R, \Delta(x) = \Delta'(0)x + \Delta(0)$ avec $\Delta(0) = \prod_{i=1}^{n}a_{i}$ et $\Delta(x) = \det_{\text{can}}(C_{1}, \ldots, C_{n})$. On a donc
      $$
        \Delta'(x) = \sum_{i=1}^{n}\vmtx{a_{1}+x & &&1&&&(x)\\&\ddots&&\vdots&&&\\&&a_{i-1}+x&&&&\\&&&1&&&\\&&&&a_{i+1}+x&&\\&&&\vdots&&\ddots&\\(x)&&&1&&&a_{n}+x}
      $$
      et
      $$
        \Delta'(0) = \sum_{i=1}^{n}\vmtx{a_{1} & &&1&&&(0)\\&\ddots&&\vdots&&&\\&&a_{i-1}&&&&\\&&&1&&&\\&&&&a_{i+1}&&\\&&&\vdots&&\ddots&\\(0)&&&1&&&a_{n}} = \sum_{i=1}^{n}\prod_{j \in \icc{1,n}\setminus \{i\}}a_{j}
      $$

    %
%

%prop
  Soient
  ~ $\varphi: I \longrightarrow \R$ dérivable en $t_{0}$  ~ $\varphi(I) \subset J$ avec $J$ un intervalle  ~ $f: J \longrightarrow E$ est dérivable en $\varphi(t_{0})$
  Alors $f\circ \varphi: I \longrightarrow E$ est un arc dérivable en $t_{0}$ et
  $$
    (f\circ \varphi)'(t_{0}) = \underbrace{\varphi'(t_{0})}_{\in \R} \underbrace{f'(\varphi(t_{0}))}_{\in E}
  $$
%

%proof
  $$
    \forall t \in J, f(t) = \sum_{i=1}^{n}f_{i}(t)e_{i}
  $$
  d’où
  $$
    \forall t \in I, f(\varphi(t)) = \sum_{i=1}^{n}f_{i}(\varphi(t))e_{i}
  $$
  On sait que $f_{i}\circ \varphi$ est dérivable en $t_{0}$ et
  $$
    (f_{i}\circ \varphi)'(t_{0}) = f_{i}'(\varphi(t_{0}))\varphi'(t_{0})
  $$
  d’où la dérivabilité de $f\circ \varphi$ en $t_{0}$ :
  $$
    \begin{align*}
      (f\circ \varphi)'(t_{0}) &= \sum_{i=1}^{n}f_{i}'(\varphi(t))\varphi'(t_{0})e_{i} \\
      &= \varphi'(t_{0})\sum_{i=1}^{n}f_{i}'(\varphi(t_{0}))e_{i}\\
      &= \varphi'(t_{0})f'(\varphi(t_{0}))
    \end{align*}
  $$
%

%def Proposition-définition
  - Un arc $f: I \longrightarrow  E$ est dit de classe $\Cont^{k}$ avec $k \in \N^{*}$ si $f$ est $k$ fois dérivable et si $f^{(k)}$ est continue sur $I$. On pose
    $$
      \Cont^{\infty}(I, E) = \bigcap_{k \in \N^{*}}\Cont^{k}(I,E)
    $$
  1. $\Cont^{k}(I, E)$ est un sous-espace vectoriel de $\Cont(I, E)$.
  2. Si $L \in \L(E,F)$ et si $f \in \Cont^{k}(I, E)$, alors $L\circ f \in \Cont^{k}(I, F)$ et $(L\circ f)^{(k)} = L\circ f^{(k)}$
  3. Si $\lambda \in \Cont^{k}(I, \K)$ et $f \in \Cont^{k}(I, E)$, alors $\lambda \cdot f \in \Cont^{k}(I, E)$ et
     $$
       (\lambda \cdot f)^{(k)} = \sum_{p=0}^{k}\binom{k}{p}\lambda^{(p)}\cdot f^{(k-p)}
     $$
  4. Si $M: E_{1} \times  \cdots \times E_{p} \longrightarrow F$ est $p$-linéaire et $g_{1} \in \Cont^{k}(I, E_{1})$, …, $g_{p} \in \Cont^{k}(I, E_{p})$, alors $M(g_{1}, \ldots, g_{p}) \in \Cont^{k}(I, F)$.
  5. Si
     ~ $\varphi: I \longrightarrow \R$ est de classe $\Cont^{k}$  ~ $\varphi(I) \subset J$ et $J$ est un intervalle  ~ $f: J \longrightarrow E$ est de classe $\Cont^{k}$
     Alors, $f\circ \varphi \in \Cont^{k}(I, E)$.
%

%proof
  Les preuves se font par récurrence sur $k$.
  5. Appelons $\Prop(k)$ l’énnoncé ci-dessus.
     - $f\circ \varphi$ est dérivable et
       $$
         \forall t \in I, (f\circ \varphi)''(t) = \varphi'(t) \cdot f'(\varphi(t))
       $$
       De plus, $\varphi' \in \Cont(I,\R)$ et $t \longmapsto f'(\varphi(t)) \in \Cont(I, E)$ donc $(f\circ \varphi)' \in \Cont(I, E)$ donc $f\circ \varphi \in \Cont^{1}(I, E)$: $\Prop(1)$ est vraie.
     - Supposons $\Prop(k)$ vraie pour $k$ fixé dans $\N^{*}$. Soient $I \xrightarrow[]{\varphi} J \xrightarrow[]{f} E$ deux applications de classe $\Cont^{k+1}$. Alors $f\circ \varphi \in \Cont^{1}$ et
       $$
         \forall t \in I, (f\circ \varphi)'(t) = \varphi'(t)f'(\varphi(t))
       $$
       or
       ~ $\varphi' \in \Cont^{k}(I, \R)$  ~ $f' \in \Cont^{k}(J, E)$  ~ $\varphi \in \Cont^{k}(I)$
       donc selon $\Prop(k)$, on a $f'\circ \varphi \in \Cont^{k}(I, E)$. D’après le troisième point, $\varphi' \times f'\circ \varphi \in \Cont^{k}(I, E)$ donc $(f\circ \varphi)' \in \Cont^{k}(I, E)$, i.e $f\circ \varphi \in \Cont^{k+1}(I, E)$.
%

%prop Formule de Taylor-Young en $t_0 \in I$ pour un arc de classe $\Cont^p$
  Si $f: I \longrightarrow E$ est un arc de classe $\Cont^{p}$ et si $t_{0} \in I$, alors il existe $\varepsilon \in \Func(I, E)$ tel que
  $$
    \forall t \in I, f(t) = f(t_{0})+(t-t_{0})f'(t_{0}) + \cdots + \frac{(t-t_{0})^{p}}{p!}f^{(p)}(t_{0}) + (t-t_{0})^{p} \varepsilon(t)
  $$
  avec $\varepsilon(t) \arrowlim{t\to t_{0}} 0_{E}$.
%

%proof
  Posons $f = \sum_{i=1}^{n}f_{i}e_{i}$. Alors $\forall i \in \icc{1,p}, f_{i} \in \Cont^{p}(I,\K)$ donc pour tout $i \in \icc{1,p}$, il existe $\varphi_{i}: I \longrightarrow \K$ tel que
  $$
    \forall t \in I, f_{i}(t) = \sum_{k=0}^{p}\frac{(t-t_{0})^{k}}{k!}f_{i}^{(k)}(t_{0}) + (t-t_{0})^{p} \varepsilon_{i}(t)
  $$
  avec $\varepsilon_{i}(t)\arrowlim{t\to t_{0}}0_{\K}$. Ainsi
  $$
    f(t) = \sum_{k=0}^{p}\frac{(t-t_{0})^{k}}{k!}f^{(k)}(t_{0}) + (t-t_{0})^{p}\varepsilon(t)
  $$
  avec $\varepsilon(t) = \sum_{i=1}^{n}\varepsilon_{i}(t)e_{i} \arrowlim{t\to t_{0}} 0_{E}$.
%

%offprog
  %eg
    Soit $f: I \longrightarrow  E$ un arc de classe $\Cont^{k}$. Soit $t_{0} \in I$. Si le quotient vectoriel $\frac{f(t) - f(t_{0})}{\norm{f(t) - f(t_{0})}}$ existe et a une limite $u$ non nulle en $t_{0}^{+}$, alors on dit que l’arc possède une demi-tangente au point $f(t_{0})$ dirigée par $u$: $f(t_{0}) + \R_{+}u$. Si $t_{0} \in \ring I$, alors on dit que l’arc possède une tangente au point $f(t_{0})$ si $\lim_{t\to t_{0}+} \frac{f(t) - f(t_{0})}{\norm{f(t)-f(t_{0})}}$ et $\lim_{t\to t_{0}-} \frac{f(t)-f(t_{0})}{\norm{f(t)-f(t_{0})}}$ ont des limites $u$ et $b$ avec $v \in \{u,-u\}$.
    ~
    L’arc dessiné ci-contre possède une tangente au point $f(t_{0})$ dirigée par le vecteur $f'(t_{0})$. Supposons $f'(t_{0})\ne 0$. On a
    $$
      \frac{f(t) - f(t_{0})}{t-t_{0}} \arrowlim{t\to t_{0}}f'(t_{0})\ne 0_{E}
    $$
    d’où
    $$
      \norm{\frac{f(t) - f(t_{0})}{t-t_{0}}}\arrowlim{t\to t_{0}}\norm{f'(t_{0})}\ne 0_{\R}
    $$
    d’où
    $$
      \frac{f(t)-f(t_{0})}{\norm{f(t) - f(t_{0})}} \arrowlim{t\to t_{0}^{\varepsilon}} \varepsilon \frac{f'(t_{0})}{\norm{f'(t_{0})}}
    $$
    avec $\varepsilon \in \{+,-\}$. Si $f'(t_{0}) = \cdots = f^{(k-1)}(t_{0}) = 0$ et $f^{(k)}(t_{0}) \ne 0$, alors
    $$
      f(t) = f(t_{0}) + \frac{(t-t_{0})^{k}}{k!}f^{(k)}(t_{0}) + (t-t_{0})^{k} \varepsilon(t)
    $$
    d’où
    $$
      \frac{f(t) - f(t_{0})}{\frac{(t-t_{0})^{k}}{k!}} \arrowlim{t\to t_{0}} f^{(k)}(t_{0})\ne 0
    $$
    donc
    $$
      \norm{\frac{f(t) - f(t_{0})}{\frac{(t-t_{0})^{k}}{k!}}} \arrowlim{t\to t_{0}} \norm{f^{(k)}(t_{0})}\ne 0
    $$
    d’où
    $$
      \frac{f(t) - f(t_{0})}{\norm{f(t) - f(t_{0})}}\arrowlim{t\to t_{0}^{\varepsilon}}\varepsilon' \frac{f^{(k)}(t_{0})}{\norm{f^{(k)}(t_{0})}}
    $$
    avec $(\varepsilon, \varepsilon') \in \{+,-\}^{2}$. L’arc possède au point $f(t_{0})$ une tangente dirigée par $f^{(k)}(t_{0})$, premier vecteur dérivé non nul en $t_{0}$.
  %
%

A partir de maintenant, tout les espaces vectoriels normés sont réels ($\K=\R$) et de dimension finie.

## Dérivée selon un vecteur

Soient $E$ et $F$ deux $\R$-espaces vectoriels normés de dimension finie. Soit $U$ une partie ouverte non vide de $E$. Soit $f: U \longrightarrow F$ une application. Soit $a \in U$.
==FIGURE 1==
Si l’arc $\varphi: t \longmapsto f(a+tv)$ est dérivable en $0$, alors on dit que $f$ possède une dérivée en $a$ selon $v$ notée $D_{v}f(a)$ égale par définition à
$$
  D_{v}f(a) = \varphi'(0) = \lim_{t\to 0}\frac{f(a+tv) - f(a)}{t}
$$

%rem
  1. - Si $v = 0$, alors $\varphi: t \longmapsto f(a+tv) = f(a)$ est définie sur $\R$.
     - Si $v\ne 0$, $U$ est ouvert et $a \in U$, donc il existe $\varepsilon > 0$ tel que $B(a, \varepsilon) \subset U$.
       $$
         \norm{a+tv - a} < \varepsilon \iff \abs{t} < \frac{\varepsilon}{\norm{v}}
       $$
       donc l’arc $\varphi$ est au moins défini sur $\oo{-\frac{\varepsilon}{\norm{v}}, \frac{\varepsilon}{\norm{v}}}$ et $0$ est dans l’intérieur de cet intervalle.
     Dans tous les cas, $\varphi$ est définie sur un intervalle d’intérieur non vide contenant $0$.
  2. $\varphi:t \longmapsto f(a)$ est dérivable en $0$ et $D_{0_{E}}f(a) = 0_{F}$
  3. Si $D_{v}f(a) \ne 0_{F}$, alors $D_{v}f(a)$ est un vecteur directeur de la tangente à l’arc $\varphi$ au point $f(a)$.
%

%eg
  - Considérons $f: \applic{\M_{n}(\R)}{\M_{n}(\R)}{M}{M^{2}}$ définie sur l’espace vectoriel normé $\M_{n}(\R)$. Soient $A \in \M_{n}(\R)$ et $M \in M_{n}(\R)$.
    $$
      \varphi:t \longmapsto f(A+tM) = (A+tM)^{2} = A^{2}+t(AM + MA) + t^{2}M^{2}
    $$
    est dérivable sur $\R$ donc en $0$, et
    $$
      \forall t \in \R, \varphi'(t) = AM+MA+2tM^{2}
    $$
    donc $D_{M}f(A) = AM+MA$.
  - %callout
      Soient $(X,Y) \in \big(\M_{n,1}(\R)\big)^{2}$. Soit $A \in \M_{n}(\R)$. Posons $X = \mtx{x_{1}\\\vdots\\x_{n}}$ et $Y = \mtx{y_{1}\\\vdots\\y_{n}}$. On pose
      $$
        \scalar{X}{Y}_{\can}  = \sum_{i=1}^{n}x_{i}y_{i} = X\transp Y = \mtx{x_{1}&\cdots&x_{n}}\mtx{y_{1}\\\vdots\\y_{n}}
      $$
      Alors
      $$
        \scalar{AX}{Y}_{\can} = (AX)\transp Y = X\transp A\transp Y = \scalar{X}{A\transp Y}_{\can}
      $$
    %
  - Soit $A \in \M_{n}(\R)$. Considérons
    $$
      g: \applic{\M_{n,1}(\R)}{\R}{X}{X\transp AX = \scalar{X}{AX}_{\can}}
    $$
    Soient $X_{0} \in \M_{n,1}(\R)$ et $V \in \M_{n,1}(\R)$.
    $$
      \varphi: t \longmapsto g(X_{0}+t V) = \scalar{X_{0} + tV}{AX_{0}+t AV}  = \scalar{X_{0}}{AX_{0}} + t \big(\scalar{V}{AX_{0}} + \scalar{X_{0}}{AV}\big) + t^{2} \scalar{V}{AV}
    $$
    est polynomiale donc dérivable en $0$ et
    $$
      \varphi'(0) = \scalar{V}{AX_{0}} + \scalar{X_{0}}{AV} = \scalar{V}{AX_{0}}  + \scalar{A\transp X_{0}}{V}
    $$
    donc
    $$
      D_{V}g(X_{0}) = \scalar{V}{(A+A\transp)X_{0}} \in \R
    $$
  - Considérons (attention, $(\R^{*})^{2} \ne \R^{2} \setminus \{(0,0)\}$)
    $$
      h: \applic{(\R^{*})^{2}}{\R}{(x,y)}{\frac{1}{xy}}
    $$
    $U = (\R^{*})^{2}$ est une partie ouverte de $\R^{2}$. Soient $(a,b) \in U$ et $(v,w) \in \R^{2}$.
    $$
      \varphi:t \longmapsto h ((a,b) + t(v,w)) = h(a+tv,b-tw) = \frac{1}{(a+tv)(b+tw)}
    $$
    est une fonction rationnelle définie sur un voisinage de $0$ donc dérivable, et
    $$
      \forall t \in V, \varphi'(t) = \frac{-v}{(a+tv)^{2}(b+tw)} - \frac{w}{(a+tv)(b+tw)^{2}}
    $$
    donc
    $$
      D_{(u,v)}h(a,b) = -\frac{v}{a^{2}b} - \frac{w}{ab^{2}}
    $$
%

%def
  Soit $(e_{1}, \ldots, e_{n}) = \B$ une base de $E$. Soit $f: U \longrightarrow F$ définie sur une partie $U$ ouverte de $E$. Soit $a \in U$.
  Si $f$ possède une dérivée en $a$ selon $e_{i}$ (pour $i \in \icc{1,n}$), alors $D_{e_{i}}f(a)$ est appelée i-ème dérivée partielle de $f$ en $a$ dans la base $\B$.
  Si aucune confusion n’est possible sur le choix de la base, alors on pose
  $$
    D_{e_{i}}f(a) = \partial_{i}f(a) = \pdv{f}{x_{i}}(a)
  $$
  si pour $x \in E$, on pose $x = \sum_{i=1}^{n}x_{i}e_{i}$.
%

%eg
  - Reprenons $f: M \longmapsto M^{2}$. Soit $(E_{i,j})_{1 \leq i,j \leq n}$ la base canonique de $\M_{n}(\R)$.
    $$
      D_{E_{i,j}}f(A) = A E_{i,j} + E_{i,j}A
    $$
    donc
    $$
      \partial_{(i,j)}f(A) = \mtx{\begin{cases}a_{jj} + a_{ii} \if (r,s) = (i,j)\\ a_{j,s} \if r=i \\a_{r,i} \if s = j \\0\else\end{cases}}_{1 \leq r,s \leq n}
    $$
  - Reprenons $g: X \longmapsto X\transp AX$. Soit $(E_{1}, \ldots, E_{n})$ la base canonique de $\M_{n,1}(\R)$.
    $$
      D_{E_{i}}g(X_{0}) = \scalar{E_{i}}{(A+A\transp)X_{0}}_{\can}
    $$
    donc
    $$
      \partial_{i}g(X_{0}) = \pdv{g}{x_{i}}(X_{0}) = \left[(A+A\transp)X_{0}\right]_{i}
    $$
  - Reprenons $h: (x,y) \longmapsto \frac{1}{xy}$ définie sur $(\R^{*})^{2}$.
    $$
      D_{(1,0)}f(a,b) = -\frac{1}{a^{2}b} = \partial_{1} f(a,b) = \pdv{f}{x}(a,b)
    $$
    donc
    $$
      D_{(0,1)}f(a,b) = -\frac{1}{ab^{2}} = \partial_{2}f(a,b) = \pdv{f}{y}(a,b)
    $$
%

De façon générale, on a pour $a = \sum_{i=1}^{n}a_{i}e_{i} \in E$ avec $\B = (e_{1}, \ldots, e_{n})$ une base de $E$,
$$
  \partial_{i}f(a) = \lim_{t\to 0}\frac{f(a+t e_{i}) - f(a)}{t} = \lim_{t\to 0}\frac{f(a_{1} e_{i} + \cdots + (a_{i} + t)e_{i} + \cdots + a_{n}e_{n})-f(a)}{t}
$$
Si note abusivement $f(x_{1}e_{1} + \cdots + x_{n}e_{n})$ par $f(x_{1}, \ldots, x_{n})$, alors
$$
  \partial_{i} f(a) = \lim_{t\to 0}\frac{f(a_{1}, \ldots, a_{i}+t, \ldots, a_{n}) - f(a_{1}, \ldots, a_{n})}{t} = \eval{\dv{}{t}\left(f(a_{1}, \ldots, a_{i}+t, \ldots, a_{n})\right)}{t=0}
$$
soit
$$
  \pdv{f}{x_{i}}(a) = \eval{\dv{}{x_{i}} \left(f(a_{1}, \ldots, x_{i}, \ldots, a_{n})\right)}{x_{i}=a_{i}}
$$

%eg
  - Soit $P \in \K[X_{1}, \ldots, X_{n}]$. Soit $E$ dont une base est $(e_{1}, \ldots, e_{n})$. Considérons
    $$
      f: \applic{E}{\K}{x = \sum_{i=1}^{n}x_{i}e_{i}}{\tilde{P}(x_{1}, \ldots, x_{n})}
    $$
    est une application polynomiale admettant une dérivée en tout point selon tout vecteur.
    Soient $a \in E$ et $v \in E$.
    $$
      \varphi: t \longmapsto f(a+tv) = \tilde{P}(a_{1}+tv_{1}, \ldots, a_{n}+tv_{n})
    $$
    est polynomiale donc dérivable sur $\R$ donc en $0$.
  - Toute fonction rationnelle en les coordonnées dans une base de $E$ possède en tout point de son ouvert de définition((toute fonction rationnelle est définie sur un ouvert, qui est l’image réciproque par son dénominateur (polynomial) de $\R$)) une dérivée selon tout vecteur. L’arc correspondant est une fonction rationnelle de la variable réelle donc dérivable au voisinage de $0$.
  - Soit $f: U \longrightarrow F$ une application définie sur un ouvert $U$. Si $V$ est un ouvert inclus dans $U$ et si $f_{|V}:V \longrightarrow F$ admet en $a \in V$ une dérivée selon $v \in E$, alors $f$ admet une dérivée en $a$ selon $v$, et
    $$
      D_{v}f(a) = D_{v}(f_{|V})(a)
    $$

    %proof
      Comme $V$ est ouvert, il existe $\varepsilon > 0$ tel que
      $$
        \forall t \in \oo{-\varepsilon, \varepsilon}, a+tv \in V \subset U
      $$
      donc
      $$
        \forall t \in \oo{-\varepsilon, \varepsilon}, \varphi(t) = f(a+tv) = f_{|V}(a+tv)
      $$
      donc $\varphi$ est dérivable en $0$ et $\varphi'(0) = D_{v}(f_{|V})(a)$ d’où
      $$
        D_{v}f(a) = D_{f}(f_{|V})(a)
      $$
    %
  - Considérons l’application
    $$
      f: \applic{\R^{2}}{\R}{(x,y)}{\begin{cases}\frac{x^{5}}{(y-x^{2})^{2}+x^{6}} \if (x,y)\ne (0,0)\\0\if (x,y)=(0,0)\end{cases}}
    $$
    On a
    $$
      (y-x^{2})^{2}+x^{6} = 0 \iff  \begin{cases}y-x^{2} = 0\\x=0\end{cases}\iff \begin{cases}x=0\\y=0\end{cases}
    $$
    donc $f$ est définie sur $\R^{2}$. $U = \R^{2} \setminus \{(0,0)\}$ est un ouvert et $f_{|U}$ est rationnelle donc possède en tout point de $U$ une dérivée selon tout vecteur de $\R^{2}$. Comme $U$ est ouvert, c’est aussi le cas pour $f$.
    - Étude en $(0,0)$ :
      - $D_{0_{\R^{2}}}f(0,0) = 0$
      - Soit $(\alpha, \beta) \in \R^{2} \setminus \{(0,0)\}$.
        $$
          t \longmapsto f((0,0) + t(\alpha, \beta)) = f(t \alpha, t \beta) = \begin{cases}\frac{t^{5}\alpha^{5}}{(t \beta - t^{2} \alpha^{2}) + t^{6} \alpha^{6}} \if t\ne 0\\ 0\if t=0\end{cases}
        $$
        - pour $\beta \ne 0$,
          $$
            \frac{t^{5} \alpha^{5}}{(t \beta - t^{2} \alpha^{2})^{2} + t^{6} \alpha^{6}} \usim{0^{\pm}} t^{3} \frac{\alpha^{5}}{\beta^{6}}
          $$
          donc, comme $f(0) = 0$, $\varphi(t) \usim 0 \frac{\alpha^{5}}{\beta^{2}}t^{3}$ i.e. $f(t) = \frac{\alpha^{5}}{\beta^{2}}t^{3} + o(t^{5}) = 0+o(t)$ donc $\varphi$ est dérivable en $0$ et $\varphi'(0) = 0$.
        - pour $\beta = 0$ (et donc $\alpha \ne 0$),
          $$
            \frac{t^{5} \alpha^{5}}{(t \beta - t^{2} \alpha^{2})^{2} + t^{6} \alpha^{6}}\usim{0^{\pm}} t \alpha
          $$
          d’où $\varphi(t)\usim 0 t \alpha$ donc $\varphi$ est dérivable en $0$ et $\varphi'(0) = \alpha$.
      Donc $f$ admet en $(0,0)$ une dérivée selon tout vecteur $(\alpha, \beta) \in \R^{2}$ et
      $$
        D_{(\alpha, \beta)}f(0,0) = \begin{cases}0 \if (\alpha, \beta) = (0,0)\\ 0 \if \beta \ne 0\\ \alpha\if \alpha\ne 0 \text{ et } \beta = 0\end{cases}
      $$
      Cependant, $f$ n’est pas continue en $(0,0)$ !
      $$
        x \longmapsto f(x,x^{2}) = \begin{cases}\frac{1}{x}\if x\ne 0\\ 0\if x=0\end{cases}
      $$
      n’est pas continue en $0$ et $(x,x^{2}) \arrowlim{x\to 0}(0,0)$.
%

## Différentiabilité

%def
  Soit $U$ un ouvert de $E$. Soit $f: U \longrightarrow F$ une application. Soit $a \in U$. On dit que $f$ est *différentiable* en $a$ si l’une des deux conditions équivalentes suivantes est satisfaite :
  1. Il existe $\ell \in \L(E,F)$ telle que
     $$
       \lim_{h\to 0_{E}} \frac{f(a+h)-f(a) - \ell \mdot h}{\norm{h}_{E}} = 0_{F}
     $$
     ce que l’on écrit encore
     $$
       f(a+h) - f(a) - \ell \mdot h \ueq{h\to 0_{E}} o(h) \ne o(\norm{h})
     $$
     ou
     $$
       f(a+h) \ueq{h\to 0_{E}} f(a) + \ell\mdot h + o(h)
     $$
  2. Il existe $\ell \in \L(E,F)$ et $\varepsilon \in \Func(U,F)$ tels que
     ~ $\forall x \in U, f(x) = f(a) + \ell\mdot (x-a) + \norm{x-a}_{E} \varepsilon(x)$  ~ $\lim_{x\to a}\varepsilon(x) = 0_{F}$
%

%proof
  - Supposons le premier point. Posons
    $$
      \varepsilon(x) = \begin{cases}\frac{f(x) - f(a) - \ell\mdot(x-a)}{\norm{x-a}_{E}} \if x\ne a \\ 0_{F} \if x=a\end{cases}
    $$
    On a alors
    $$
      \forall x \in U, f(x) = f(a) + \ell\mdot (x-a) + \norm{x-a}\varepsilon(x)
    $$
    et $\lim_{x\to a}\varepsilon(x) = \lim_{h\to 0_{E}}\varepsilon(a+h) = 0_{F}$.
  - Supposons le second point. Il existe $\delta > 0$ tel que $B_{E}(a, \delta) \subset U$. Soit $h \in B_{E}(0_{t}e, \delta)$.
    $$
      f(a+h) = f(a) + \ell\mdot h + \norm{h}\varepsilon(a+h)
    $$
    d’où pour $h \in B_{E}(0_{t}e, \delta) \setminus \{0_{E}\}$,
    $$
      \frac{f(a+h) - f(a) - \ell \mdot h}{\norm{h}_{E}} = \varepsilon(a+h) \arrowlim{h\to 0_{E}}0_{F}
    $$
%

%rem
  La notion de différentiabilité ne dépend pas des normes choisies sur $E$ et $F'$. Soient
  $$
    \alpha \norm{\cdot}_{E}' \leq \norm{\cdot}_{E} \leq \beta \norm{\cdot}_{E}
  $$
  et
  $$
    \gamma \norm{\cdot}_{F}' \leq  \norm{\cdot}_{F} \leq \delta \norm{\cdot}_{F}'
  $$
  avec $\alpha$, $\beta$, $\gamma$ et $\delta$ des réels strictement positifs.
  $$
    \frac{\gamma}{\beta}\frac{\norm{\cdot}_{F}'}{\norm{h}_{E}} \leq \frac{\norm{\cdot}_{F}}{\norm{h}_{E}} \leq  \frac{\delta}{\alpha}\frac{\norm{\cdot}_{F}'}{\norm{h}_{E}'}
  $$
%

%prop
  Soit $f: U \longrightarrow F$ une application. Soit $a \in U$.
  - Si $\ell \in \L(E,F)$ vérifie
    $$
      \lim_{h\to 0} \frac{f(a+h)-f(a) - \ell\mdot h}{\norm{h}_{E}} = 0_{F}
    $$
    alors $\ell$ est unique.
  - Si $f$ est différentiable en $a$, alors l’unique application linéaire $\ell' \in \L(E,F)$ telle que
    $$
      \lim_{h\to 0_{E}}\frac{f(a+h)-f(a) - \ell\mdot h}{\norm{h}_{E}} = 0_{F}
    $$
    est appelée différentielle de $f$ en $a$ et notée $\dd f(a)$. Ainsi,
    $$
      \boxed{\begin{cases}\dd f(a) \in \L(E,F)\\ f(x) = f(a) + \dd f(a) \mdot (x-a) + \norm{x-a}\varepsilon(x)\\ \varepsilon(x) \arrowlim{x\to a}0_{F}\end{cases}}
    $$
%

%proof
  1. Soient $(\ell, \ell') \in (\L(E,F))^{2}$. Supposons
     $$
       \lim_{h\to 0_{E}}\frac{f(a+h)-f(a)-\ell\mdot h}{\norm{h}_{E}} = 0_{F} = \lim_{h\to 0_{E}}\frac{f(a+h) - f(a) - \ell'\mdot h}{\norm{h}_{E}}
     $$
     d’où par différence,
     $$
       \lim_{h\to 0_{E}}\frac{(\ell-\ell')\mdot h}{\norm{h}_{E}} = 0_{E}
     $$
     Soit $u \in E \setminus \{0\}$. $\lim_{t\to 0^{+}}tu = 0_{E}$ donc par composition,
     $$
       \lim_{t\to 0^{+}}\frac{(\ell-\ell')\mdot (tu)}{\norm{tu}_{E}} = 0_{F}
     $$
     i.e. $\lim_{t\to 0^{+}}\frac{(\ell-\ell')\mdot u}{\norm{u}_{E}} = 0_{F}$ donc
     $$
       \frac{(\ell-\ell')\mdot u}{\norm{u_{E}}}=0_{F}
     $$
     i.e. $\ell(u) = \ell'(u)$ (vrai aussi pour $u = 0_{E}$).
%

%eg
  - Reprenons $f: \applic{\M_{n}(\R)}{\M_{n}(\R)}{M}{M^{2}}$. Soit $A \in \M_{n}(\R)$. Soit $H \in \M_{n}(\R)$.
    $$
      f(A+H) = (A+H)^{2} = A^{2} + AH + HA + H^{2}
    $$
    et $L: \applic{\M_{n}(\R)}{\M_{n}(\R)}{H}{AH+HA}$ est linéaire. Soit $\norm{\cdot}$ une norme sous-multiplicative sur $\M_{n}(\R)$. Alors $\norm{H^{2}} \leq \norm{H}^{2}$
    $$
      \frac{\norm{H^{2}}}{\norm{H}} \leq \norm{H} \arrowlim{}0
    $$
    donc $\frac{H^{2}}{\norm{H}} = \varepsilon(H)\arrowlim{H\to 0_{n}} 0_{n}$ donc $H^{2} \ueq{H\to 0_{n}}o(H)$. Ainsi, $f$ est différentiable en $A$ et
    $$
      \dd f(A): \applic{\M_{n}(\R)}{\M_{n}(\R)}{H}{AH+HA}
    $$
    (cette application est linéaire).
  - Soit $A \in \M_{n}(\R)$. Considérons
    $$
      g: \applic{\M_{n,1}(\R)}{\R}{X}{X\transp AX}
    $$
    Soient $X_{0} \in \M_{n,1}(\R)$ et $H \in \M_{n,1}(\R)$.
    $$
      g(X_{0} + H) = \scalar{X_{0}+H}{AX_{0}+AH} = \scalar{X_{0}}{AX} +\scalar{H}{AX_{0}}  + \scalar{X_{0}}{AH}  + \scalar{H}{AH}
    $$
    donc $H \longmapsto \scalar{H}{AX_{0}} +\scalar{X_{0}}{AH}  = \scalar{H}{(A+A\transp)X_{0}}$ est une forme linéaire sur $\M_{n}(\R)$.
    $$
      \abs{\scalar{H}{AH} } \leq \norm{H}\times \norm{AH}
    $$
    or $\norm{AH}\arrowlim{H\to 0_{n,1}}0$ d’où $\scalar{H}{AH} \ueq{H\to 0}o(H)$ donc $g$ est différentiable en $X_{0}$ et
    $$
      \dd g(X_{0}): \applic{\M_{n}(\R)}{\R}{H}{\scalar{H}{(A+A\transp)X_{0}}}
    $$
  - Soit $N: E \longrightarrow \R_{+}$ une norme sur $E$. Alors $N$ n’est pas différentiable en $0_{E}$.
    ~
    Supposons $N$ différentiable en $0_{E}$. Il existe alors $\ell \in \L(E,\R)$ telle que
    $$
      \frac{N(h) - N(0)-\ell\mdot h}{N(h)} \arrowlim{h\to 0_{E}}0
    $$
    i.e.$1-\ell\mdot \left(\frac{h}{N(h)}\right) \arrowlim{h\to 0_{E}}0$ donc
    $$
      \ell\mdot \left(\frac{h}{N(h)}\right)\arrowlim{h\to 0_{E}}1
    $$
    Soit $u \in E \setminus \{0_{E}\}$. $tu \arrowlim{t\to 0^{\pm}}O_{E}$ donc
    $$
      \lim_{t\to 0^{+}}\ell\mdot \left(\frac{tu}{N(t)u}\right) = \lim_{t\to 0^{-}}\ell\mdot \left(\frac{tu}{N(tu)}\right)
    $$
    donc $\ell\mdot \left(\frac{u}{N(u)}\right) = -\ell\mdot \left(\frac{u}{N(u)}\right) = 1$, ce qui est impossible.
%

%property
  Soient $U$ un ouvert de $E$ et $a$ un point de $E$.
  1. Si $f: U \longrightarrow F$ est différentiable en $a$, alors
     - $f$ est continue en $a$
     - $f$ admet une dérivée en $a$ selon tout vecteur $v \in E$ et
       $$
         D_{v}f(a) = \dd f(a) \mdot v
       $$
       et si $v = \sum_{i=1}^{n}v_{i}e_{i}$, alors
       $$
         \dd f(a) \mdot v = \sum_{i=1}^{n}v_{i} \partial_{i}f(a)
       $$
  2. Si $f: U \longrightarrow F$ est constante, alors $f$ est différentiable en tout point $x \in U$ et
     $$
       \dd f(x) = 0_{\L(E,F)}
     $$
  %callout
    3. Si $L \in \L(E,F)$ et si $\ell = L_{|U}$, alors $\ell$ est différentiable en tout point $x$ de $U$ et $\dd \ell(x) = L$.
  %
  4. Si $f: U \longrightarrow F$ et $g: U \longrightarrow F$ sont différentiables en $a$ et si $(\lambda, \mu) \in \R^{2}$, alors $\lambda f + \mu g$ est différentiable en $a$ et
     $$
       \dd (\lambda f+\mu g) (a) = \lambda \dd f(a) + \mu \dd g(a)
     $$
  5. Soit $I$ un intervalle réel ouvert non vide. Soit $f: I \longrightarrow F$ un arc. Pour tout $t_{0} \in I = \ring I$,
     $$
       f \text{ dérivable en } t_{0} \iff f \text{ différentiable en } t_{0}
     $$
     Si tel est le cas, alors
     $$
       f'(t_{0}) = \dd f(t_{0})\mdot 1
     $$
%

%proof
  1. Soit $f: U \longrightarrow F$ différentiable en $a$.
     - On a
       $$
         f(a+h) = f(a) + \dd f(a)\mdot h + \norm{h}\varepsilon(h)
       $$
       où $\varepsilon \in \Func(B(0_{E}, \delta), F)$ est tel que  $\varepsilon(h)\arrowlim{h\to 0_{E}}0_{F}$. Ainsi,
       $$
         f(a+h) \arrowlim{h\to 0_{E}}f(a)
       $$
       donc $f$ est continue en $a$.
     - Soit $v \in E$. Il existe $\varepsilon > 0$ tel que
       $$
         \forall t \in \oo{-\varepsilon, \varepsilon}, tv \in B(0_{E}, \delta)
       $$
       d’où
       $$
         \forall t \in \oo{-\varepsilon, \varepsilon}, \underbrace{f(a+tv)}_{\varphi(t)} = f(a) + t \dd f(a)\mdot v + \abs{t}\norm{v}\varepsilon(tv)
       $$
       soit
       $$
         \varphi(t) \ueq{t\to 0} \varphi(0)+t \underbrace{\dd f(a)\mdot v}_{\in F} + o(t)
       $$
       $\varphi$ admet donc un développement limité d’ordre $1$ en $0$, donc elle est dérivable en $0$. Autrement dit, $f$ admet en $a$ une dérivée selon $v$ et
       $$
         D_{v}f(a) = \varphi'(0) = \dd f(a)\mdot v
       $$
       Si $f = \sum_{i=1}^{n}v_{i}e_{i}$, alors
       $$
         \dd f(a)\mdot v = \sum_{i=1}^{n}v_{i}\dd f(a)\mdot e_{i} f \sum_{i=1}^{n}v_{i}D_{e_{i}}f(a)
       $$
       donc $\dd f(a)\mdot v = \sum_{i=1}^{n}f_{i}\partial_{i}f(a)$
  2. Si $f$ est constante, pour $a \in U$,
     $$
       \forall x \in U, f(x) = f(a) + 0_{\L(E,F)}(x-a) + \norm{x-a}\varepsilon(x)
     $$
     avec $\varepsilon : \applic{U}{F}{x}{0_{F}}$. Comme $0_{\L(E,F) \in \L(E,F)}$, on a $\dd f(a) = 0_{\L(E,F)}$.
  3. $L \in \L(E,F)$ et $\ell \in \L_{|U}$. Soit $a \in U$.
     $$
       \forall x \in U, \ell(x) = L(x) = L(a) + L(x-a)
     $$
     d’où
     $$
       \ell(x) = \ell(a) + L\mdot(x-a) + \norm{x-a}\varepsilon(x)
     $$
     avec $\varepsilon(x)=0_{F} \arrowlim{x\to a}0_{F}$. Comme $L \in \L(E,F)$, on a $\dd \ell(a) = L$.
  4. $f(a+h) = f(a) + \dd f(a)\mdot h + \norm{h}\varepsilon_{1}(h)$ et $g(a+h) = g(a) + \dd g(a)\mdot h + \norm{h}\varepsilon_{2}(h)$ avec
     ~  $\varepsilon_{1}(h)\arrowlim{h\to 0_{E}}0_{F}$  ~ $\varepsilon_{2}(h)\arrowlim{h\to 0_{E}}0_{F}$
     d’où
     $$
       (\lambda f+\mu g)(a+h) = \lambda f(a+h)+\mu g(a+h) = (\lambda f(a) + \mu g(a)) + (\lambda \dd f(a) + \mu \dd g(a))\mdot h + \norm{h}\varepsilon(h)
     $$
     avec $\varepsilon(h) = \lambda \varepsilon_{1}(h) + \mu \varepsilon_{2}(h) \arrowlim{h\to 0_{E}}0_{F}$. Or
     $$
       \lambda \dd f(a) + \mu \dd  g(a) \in \L(E,F)
     $$
     d’où la conclusion.
  5. Soient $f: I \longrightarrow F$ avec $I$ un intervalle ouvert et $t_{0}$ un point de $\ring I = I$.
     %recall
       Si $L \in \L(\R,F)$, alors $\forall h \in \R, L(h) = hL(1)$ donc
       $$
         \L(\R,F) = \left\{\applic{\R}{F}{h}{hu}\where u \in F\right\}
       $$
     %
     $f$ est dérivable en $t_{0}$ si et seulement si
     $$
       \exists u \in F: \lim_{h\to 0}\left[\frac{f(t_{0}+h)-f(t_{0})}{h} - u\right] = 0_{F}
     $$
     donc si et seulement si
     $$
       \exists u \in F: \lim_{h\to 0} \left[\frac{f(t_{0}+h) - f(t_{0}) - hu}{h}\right] = 0_{F}
     $$
     soit si et seulement si
     $$
       \exists u \in F: \lim_{h\to 0} \left[\frac{f(t_{0}+h) - f(t_{0}) - hu}{\abs{h}}\right] = 0_{F}
     $$
     ce qui est le cas si et seulement si $f$ est différentiable en $t_{0}$.
     En cas de dérivabilité, on a
     $$
       \dd f(t_{0}): \applic{\R}{F}{h}{h f'(t_{0}0)}
     $$
     soit
     $$
       \dd f(t_{0})\mdot 1 = f'(t_{0})
     $$
%

%prop Différentiabilité d’une application multilinéaire
  Si $M: E_{1} \times  \cdots \times E_{p} \longrightarrow F$ est une application $p$-linéaire, alors $M$ est différentiable en tout point $(a_{1}, \ldots, a_{p})\in E_{1} \times  \cdots \times E_{p}$ et
  $$
    \dd M(a_{1}, \ldots, a_{p}): \applic{E_{1} \times  \cdots \times E_{p}}{F}{(h1, \ldots, h_{p})}{\displaystyle\sum_{i=1}^{p}M(a_{1}, \ldots, a_{i-1}, h_{i}, a_{i+1}, \ldots, a_{p})}
  $$
%

%proof
  Soit $(a_{1}, \ldots, a_{p})\in E_{1} \times  \cdots \times E_{p}$.
  $$
    M(a_{1}+h_{1}, \ldots, a_{p}+h_{p}) - M(a_{1}, \ldots, a_{p}) = \sum_{i=1}^{p}M(a_{1}, \ldots, a_{i-1}, \underbrace{a_{i}+h_{i}-a_{i}}_{h_{i}}, a_{i+1}+h_{i+1}, \ldots, a_{p}+h_{p})
  $$
  donc
  $$
    M(a+h)-M(a) = \sum_{i=1}^{p}M(a_{1}, \ldots, a_{i-1}, h_{i}, a_{i+1}, \ldots, a_{p}) + S
  $$
  où $S$ est une somme finie de termes de la forme $M(a'_{1}, \ldots, a'_{p})$ avec $\forall i \in \icc{1,p}, a'_{i} \in \{a_{i}, h_{i}\}$ avec
  $$
    \exists (i,j) \in \icc{1,p}^{2}: i\ne j \text{ et } a'_{i} = h_{i} \text{ et }a'_{j} = h_{j}
  $$
  donc
  $$
    \norm{M(a'_{1}, \ldots, a'_{p})}_{F} \leq K \norm{a'_{1}}_{E_{1}}\times  \cdots \times  \norm{a'_{p}}_{E_{p}} \leq K N_{\infty}(h_{1}, \ldots, h_{p})^{2} \times  N_{\infty}(h_{1}, \ldots, h_{p})^{p-2-r} \times N_{\infty}(a_{1}, \ldots, a_{p})^{r}
  $$
  et
  $$
    \frac{M(a'_{1}, \ldots, a'_{p})}{N_{\infty}(h_{1}, \ldots, h_{p})}\arrowlim{(h_{1}, \ldots, h_{p})\to (0_{E_{1} \times  \cdots \times E_{p}})}0_{F}
  $$
  donc $S = o \big((h_{1}, \ldots, h_{p})\big)$.
%

%eg
  - Soit $E$ un espace euclidien. Considérons l’application bilinéaire
    $$
      \pi: \applic{E^{2}}{\R}{(x,y)}{\scalar{x}{y} }
    $$
    Elle est donc différentiable et pour tout $(x,y) \in E^{2}$,
    $$
      \dd \pi(x,y): \applic{E^{2}}{\R}{(h,k)}{\scalar{h}{y} + \scalar{x}{k}}
    $$
  - Soit $E$ un $\R$-espace vectoriel de dimension 2. Soit $\B$ une base de $E$. $\det_{\B}: E^{n} \longrightarrow \R$ est $n$-linéaire donc différentiable en tout $(u_{1}, \ldots, u_{n}) \in E^{n}$ et
    $$
      \dd (\det_{\B})(u_{1}, \ldots, u_{n}): \applic{E^{n}}{\R}{(h_{1}, \ldots, h_{n})}{\sum_{i=1}^{n}\det_{\B}}(u_{1}, \ldots, u_{i-1}h_{i}, u_{i+1}, \ldots, u_{n})
    $$
%

%prop Dérivée le long d’un arc
  Soit $f: U \longrightarrow F$ différentiable en tout point de $U$. Soit $\gamma: I \longrightarrow  E$ un arc tracé sur $U$ et dérivable sur un intervalle $I$ d’intérieur non vide.
  %fig Dérivation le long d’un arc
    @[./figures/dérivation-le-long-d-un-arc_optimized.svg]
  %
  Alors l’arc $f\circ \gamma: I \longrightarrow F$ est dérivable sur $I$ et
  $$
    \forall t \in I, \underbrace{(f\circ \gamma)'(t)}_{\in F} = \underbrace{\dd f(\gamma(t))}_{\in \L(E,F)}\mdot \underbrace{\gamma'(t)}_{\in E}
  $$
  ($\dd f(\gamma(t))$ est l’application linéaire tangente à $f$ en $\gamma(t)$, ou différentielle de $f$ en $\gamma(t)$).
%

%proof
  Soit $t_{0} \in I$. $f$ est différentiable en $\gamma(t_{0}) \in U$ donc
  $$
    \forall x \in U, f(x) = f(\gamma(t_{0})) + \dd f(\gamma(t_{0}))\mdot (x-\gamma(t_{0})) + \norm{x-\gamma(t_{0})}\varepsilon(x)
  $$
  avec $\varepsilon(x) \arrowlim{x\to \gamma(t_{0})}0_{F}$. Or $\forall t \in I, \gamma(t) \in U$ donc
  $$
    \forall t \in I, f(\gamma(t)) = f(\gamma(t_{0})) + \dd f(\gamma(t_{0}))\mdot (\gamma(t) - \gamma(t_{0})) + \norm{\gamma(t) - \gamma(t_{0})}\varepsilon(\gamma(t))
  $$
  Soit $t \in I \setminus \{t_{0}\}$.
  $$
    \frac{f(\gamma(t)) - f(\gamma(t_{0}))}{t-t_{0}} = \underbrace{\dd f(\gamma(t_{0}))\mdot \frac{\gamma(t)-\gamma(t_{0})}{t-t_{0}}}_{\arrowlim{t\to t_{0}}\dd f(\gamma(t_{0}))\mdot \gamma'(t_{0})} + \underbrace{\frac{\norm{\gamma(t) - \gamma(t_{0})}}{t-t_{0}}}_{\arrowlim{t\to t_{0}^{\pm}}\norm{\gamma'(t_{0})}}\underbrace{\varepsilon(\gamma(t))}_{\arrowlim{t\to t_{0}}0_{F}}
  $$
  donc $t \longmapsto (f\circ \gamma)(t)$ est dérivable en $t_{0}$ et
  $$
    (f\circ \gamma)'(t_{0}) = \dd f(\gamma(t_{0}))\mdot \gamma'(t_{0})
  $$
%

%eg
  - Si $\cc{a,b} \subset U$ et $f: U \longrightarrow F$ est différentiable, alors $\gamma: t \in \cc{0,1} \longmapsto a+t(b-a)$ est tracée sur $U$ et dérivable donc
    $$
      \dv{}{t}\left(f(a+t(b-a))\right) = \dd f \big(a+t(b-a)\big) \mdot (b-a)
    $$
  - Si $E$ a pour base $\B = (e_{1}, \ldots, e_{n})$, et si
    $$
      \forall t \in I, \gamma(t) = \sum_{i=1}^{n}\gamma_{i}(t)e_{i}
    $$
    alors pour $f: U \longrightarrow F$ différentiable sur $U$ et $\gamma$ différentiable sur $I$,
    $$
      \dv{}{t}\big((f\circ \gamma)(t)\big) = \dd f(\gamma(t))\mdot \sum_{i=1}^{n}\gamma_{i}'(t)e_{i} = \sum_{i=1}^{n}\gamma_{i}'(t)\underbrace{\dd f(\gamma(t))\mdot e_{i}}_{D_{e_{i}}f(\gamma(t))}
    $$
    d’où
    $$
      \dv{}{t}\big(f(\gamma(t))\big) = \sum_{i=1}^{n}\gamma_{i}'(t)\partial_{i}f(\gamma(t))
    $$
    ou encore si $\gamma: t \longmapsto (x_{1}(t), \ldots, x_{n}(t)) \in \R^{n}$ et $f: U \subset \R^{n} \longrightarrow \R$ est différentiable, alors
    $$
      \dv{}{t}\big(f(x_{1}(t), \ldots, x_{n}(t))\big) = \sum_{i=1}^{n}\underbrace{x'_{i}(t) \pdv{f}{x_{i}}\big((x_{1}(t), \ldots, x_{n}(t))\big)}_{\pdv{f}{x_{i}}\big(x_{1}(t), \ldots, x_{n}(t)\big)\dv{x_{i}}{t}}
    $$
%

%prop
  Soit $U$ un ouvert non vide convexe par arcs de $E$. Soit $f: U \longrightarrow F$ une application. Alors $f$ est constante si et seulement si
  ~ $f: U \longrightarrow F$ est différentiable sur $U$  ~ $\forall x \in U, \dd f(x) = 0_{\L(E,F)}$
%

%proof
  - Le sens direct est connu
  - Supposons $f:U \longrightarrow F$ différentiable sur $U$ connexe par arcs et telle que $\forall x \in U, \dd f(x) = 0_{\L(E,F)}$.
    - Si $U$ est convexe, soit $(a,b) \in U^{2}$. Comme $U$ est convexe, on a $\cc{a,b} \subset U$. $\gamma: \applic{\cc{0,1}}{U}{t}{a+t(b-a)}$ est dérivable sur $\cc{0,1}$ donc $t \longmapsto (f\circ \gamma)(t)$ est dérivable sur $\cc{0,1}$ et
      $$
        \forall t \in \cc{0,1}, (f\circ \gamma)'(t) = \dd f(\gamma(t)) \mdot \gamma'(t) = 0_{\L(E,F)}\mdot \gamma'(t) = 0
      $$
      donc $f\circ \gamma$ est un arc constant, donc $f(\gamma(1)) = f(\gamma(0))$, i.e. $f(b) = f(a)$.
    - Sinon, $U$ est connexe par arcs. Soit $a \in U$. Comme $U$ est ouvert, il existe $\varepsilon > 0$ tel que $B(a, \varepsilon) \subset U$. On a $\forall x \in U, \dd f(x) = 0$ donc $\forall x \in B(a, \varepsilon), \dd f(x) = 0$ or $B(a, \varepsilon)$ est convexe, donc $f$ est constante sur $B(a, \varepsilon)$. $f$ est donc localement constante sur $U$ convexe par arcs, donc (exercice 11), $f$ est constante sur $U$.
%

%term
  Si $f: U \longrightarrow F$ est différentiable en tout point de $U$, alors
  $$
    \dd f: \applic{U}{\L(E,F)}{x}{\dd f(x)}
  $$
  est appelée *différentielle de $f$*. C’est un « objet compliqué ». On a alors pour $v = \sum_{i=1}^{n}v_{i}e_{i}$ où $(e_{1}, \ldots, e_{n})$ est une base de $E$ :
  $$
    \forall x \in U, \dd f(x) \mdot v = \sum_{i=1}^{n}v_{i}\dd f(x)\mdot e_{i} = \sum_{i=1}^{n}v_{i} \partial_{i}f(x)
  $$
  $e_{i}^{*}: \applic{E}{\R}{x = \sum_{i=1}^{n}x_{i}e_{i}}{x_{i}}$ est linéaire donc différentiable en tout point et
  $$
    \forall x \in E, \dd e_{i}^{*}(x) = e_{i}^{*}
  $$
  donc
  $$
    \forall (x,v) \in E, \dd e_{i}^{*}(x)\mdot v = v_{i}
  $$
  donc
  $$
    \underbrace{\dd f(x)\mdot v}_{\in F} = \sum_{i=1}^{n}\underbrace{\dd e_{i}^{*}(x)\mdot v}_{\in \R}\underbrace{\partial_{i}f(x)}_{\in F}
  $$
  Si $F =\R$ ou $F = \C$,
  $$
    \forall v \in E, \dd f(x)\mdot v = \sum_{i=1}^{n}\partial_{i}f(x)\dd e_{i}^{*}(x)\mdot v
  $$
  et
  $$
    \forall x \in U, \underbrace{\dd f(x)}_{\in \L(E,\R)} = \sum_{i=1}^{n}\partial_{i}f(x)\underbrace{\dd e_{i}^{*}(x)}_{\L(E,\R)}
  $$
  ce qu’on écrit
  $$
    \dd f = \sum_{i=1}^{n}\partial_{i}f \dd e_{i}^{*} \sur U
  $$
  ou
  $$
    \dd f = \sum_{i=1}^{n}\frac{\partial f}{\partial x_{i}}\dd x_{i}
  $$
  À interpréter comme
  $$
    \forall x \in U, \dd f(x) = \sum_{i=1}^{n}\pdv{f}{x_{i}}(x)\dd x_{i}(x)
  $$
  et
  $$
    \forall x \in U, \forall h \in E, \dd f(x)\mdot h = \sum_{i=1}^{n}h_{i} \pdv{f}{x_{i}}(x)
  $$
  $$
    f(a+h) = f(a) + \sum_{i=1}^{n}h_{i} \pdv{f}{x_{i}}(a) + \norm{h}\varepsilon(h)
  $$
  avec $\varepsilon(h)\arrowlim{h\to 0_{E}}0$.
  $$
    f(a+h)-f(a) = \Delta f(a) = \sum_{i=1}^{x_{i}}\pdv{f}{x_{i}}\Delta x_{i}
  $$
%

## Composition des applications différentiables

%prop
  Soient $E$, $F$ et $G$ trois $\R$-espaces vectoriels normés de dimensions finies. Soient
  ~ $f: U \longrightarrow F$ différentiable en $a \in U$, $U$ ouvert de $E$
  ~ $f(U)\subset V$ avec $V$ ouvert de $E$
  ~ $g: V \longrightarrow G$ différentiable en $f(a)\in V$
  Alors, $g\circ f \longrightarrow G$ est différentiable en $a$ et
  $$
    \underbrace{\dd (g\circ f)(a)}_{\in \L(E,G)} = \underbrace{\dd g(f(a))}_{\in \L(F,G)}\circ \underbrace{\dd f(a)}_{\in \L(E,F)}
  $$
%

%proof
  $$
    f(a+h) = f(a) + \dd f(a)\mdot h + \norm{h}_{E} \varepsilon_{1}(h)
  $$
  avec $\varepsilon_{1}: B_{E}(0_{F}, \delta_{2}) \longrightarrow G$ et $\lim_{x\to 0_{E}}\varepsilon_{1}(x) = 0_{F}$, donc
  $$
    g(f(a)+h) = g(f(a)) + \dd g(f(a))\mdot h + \norm{h}_{F}\varepsilon_{2}(h)
  $$
  avce $\varepsilon_{2}: B_{F}(0_{F}, \delta_{2}) \longrightarrow G$ vérifiant $\lim_{x\to 0_{F}}\varepsilon_{2}(x) = 0_{G}$. Or $\dd f(a)\mdot h + \norm{h}_{E} \varepsilon_{1}(h) \arrowlim{h\to 0_{E}}0_{F}$ donc il existe $\delta \in \oo{0,\delta_{1}}$ tel que
  $$
    \forall h \in B_{E}(0_{E}, \delta), K = \dd f(a)\mdot h+\norm{h}_{E} \varepsilon_{1}(h) \in B_{F}(0_{F}, \delta_{2})
  $$
  D’où, pour tot $h \in B_{E}(0_{E}, \delta)$,
  $$
    \begin{align*}
      (g\circ f)(a+h) &= g(f(a+h))\\
       &= g(f(a) + K)\\
        &= g(f(a)) + \dd g(f(a))\mdot K + \norm{K}_{F}\varepsilon_{2}(K)\\
        &= g(f(a)) + \underbrace{\dd g(f(a))\mdot \big(\dd f(a)\mdot h\big)}_{\big(\dd g(f(a))\circ \dd f(a)\big)\mdot h}+ \underbrace{\norm{h}_{E} \dd g(f(a))\mdot \varepsilon_{1}(h)}_{o(h)} + \norm{K}_{F} \varepsilon_{2}(K)
    \end{align*}
  $$
  or
  $$
    \norm{K}_{F} \norm{\varepsilon_{2}(K)}_{G} \leq \big(\underbrace{\opnorm{\dd f(a)} \norm{h}_{E} + \norm{h}_{E} \norm{\varepsilon_{1}(h)}_{F}}_{\norm{h} \big(\opnorm{\dd f(a)} + \norm{\varepsilon_{1}(h)}_{F}\big)}\big)\underbrace{\norm{\varepsilon_{2}(K)}_{G}}_{\arrowlim{h\to 0_{E}}0} = \norm{h} \eta (h)
  $$
  avec $\eta(h)\arrowlim{h\to 0_{E}}0$. Ainsi, $\norm{K}\varepsilon_{2}(K) \ueq{h\to 0_{F}} o(h)$.
%

%eg
  - Considérons $g: \applic{\M_{n,1}(\R)}{\R}{X}{X\transp AX = \scalar{X}{AX}} = \pi\circ L$ avec $L: X \longmapsto (X,AX)$ et $\pi = \scalar{\cdot}{\cdot}$. $\pi$ est bilinéaire donc différentiable sur $\M_{n,1}(\R)^{2}$ et $L$ est bilinéaire donc différentiable sur $\M_{n,1}(\R)$. Par compositions, $g$ est différentiable sur $\M_{n,1}(\R)$ et
    $$
      \forall X \in \M_{n,1}(\R), \dd g(X) = \dd (\pi\circ L)(X) = \dd  \pi(L(X))\circ \dd L(X) = \dd  \pi(X,AX)\circ L
    $$
    d’où
    $$
      \begin{align*}
        \forall (X,H) \in \M_{n,1}(\R)^{2}, \dd g(X)\mdot H &= \dd  \pi(X,AX) \mdot (L(H))\\
          &= \dd \pi(X,AX)\mdot (H,AH) \\
          &= \pi(H,AX) + \pi(X,AH) \\
          &= \scalar{H}{AX}  + \scalar{X}{AH} \\
          &= \scalar{H}{(A+A\transp)X}
      \end{align*}
    $$
  - Soit $f: U \longrightarrow \R$ une fonction différentiable sur un ouvert $U$ de $E$ à valeurs dans un intervalle ouvert $I$ de $\R$. Soit $\varepsilon: I \longrightarrow \R$ dérivable sur $I$. Considérons $\varphi\circ f: U \longrightarrow \R$. Cette fonction est différentiable sur $U$ et
    $$
      \forall x \in U, \dd (\varphi\circ f)(x) = \dd \varphi(f(x))\circ \dd f(x)
    $$
    donc pour tout $x \in U$ et pour tout $h \in E$,
    $$
      \dd (\varphi\circ f)(x)\mdot h = \dd \varphi(f(x))\mdot \big(\underbrace{\dd f(x)\mdot h}_{\in \R}\big) = (\underbrace{\dd f(x)\mdot h}_{\in \R}) \underbrace{\varphi'(f(x))}_{\in \R} = \big(\varphi'(f(x))\big)(\dd f(x)\mdot h)
    $$
    d’où
    $$
      \forall x \in U, \underbrace{\dd (\varphi\circ f)(x)}_{\L(E,\R)} = \underbrace{(\varphi'\circ f)(x)}_{\in \R} \underbrace{\dd f(x)}_{\in \L(E,\R)}
    $$
    d’où $\dd (\varphi\circ f) = \varphi'\circ f \dd f$ sur $U$, ce qu’on écrit
    $$
      \dd (\varphi(f)) = \varphi'(f)\dd f
    $$
    %eg
      Par exemple, $\dd (e^{f}) = e^{f}\dd f$ et $\dd (\sqrt{f}) = \frac{1}{2 \sqrt{f}}\dd f$ si $f: U \rightarrow \R_{+}^{*}$. Formellement, cela revient à
      $$
        \dd (\sqrt{f})(x) = \frac{1}{2 \sqrt{f(x)}}\dd f(x)
      $$
      d’où
      $$
        \dd (\sqrt{f})(x)\mdot h = \frac{1}{2 \sqrt{f(x)}}\dd f(x)\mdot h
      $$
    %
%

%prop
  1. Soit $f: \applic{U}{F_{1} \times  \cdots \times F_{q}}{x}{(f_{1}(x), \ldots, f_{q}(x))}$ définie sur un ouvert $U$ de $E$. $f$ est différentiable en $a \in U$ si et seulement si pour tout $i \in \icc{1,q}$, $f_{i}$ est différentiable en $a$. En cas de différentiabilité,
     $$
       \forall h \in E, \underbrace{\dd f(a)\mdot h}_{\in F_{1} \times  \cdots \times F_{q}} = \big(\underbrace{\dd f_{1}(a)\mdot h}_{\in F_{1}}, \ldots, \underbrace{\dd f_{q}(a)\mdot h}_{\in F_{q}}\big)
     $$
  2. Soit $f: \applic{U}{F}{x}{\sum_{i=1}^{q}f_{i}(x)e_{i}}$ avec $(e_{1}, \ldots, e_{q})$ une base de $E$ et $f_{i}$ à valeurs dans $\R$. $f$ est différentiable en $a \in U$ st et seulement si pour tout $i \in \icc{1,q}$, $f_{i}$ est différentiable en $a$. En cas de différentiabilité, on a
     $$
       \forall h \in E, \dd f(a)\mdot h = \sum_{i=1}^{q}\big(\underbrace{\dd f_{i}(a)\mdot h}_{\in \R}\big)e_{i}
     $$
%

%proof
  1. - Notons $\pi_{i}: F_{1} \times  \cdots \times F_{q} \longrightarrow F_{i}$ la i-ème projection sur le facteur $F_{i}$. Elle est linéaire donc différentiable et $f_{i} = \pi_{i}\circ f$. Ainsi, si $f$ est différentiable en $a$, $f_{i}$ est différentiable en $a$.
     - Supposons chaque $f_{i}$ différentiable en $a$. Alors
       $$
         \forall x \in U, f(x) = \big(f_{i}(a)+\dd f_{i}(a)\mdot (x-a) + \norm{x-a}\varepsilon_{i}(x)\big)_{1 \leq i \leq q} = f(a) + L\mdot (x-a) + \norm{x-a}\varepsilon(x)
       $$
       avec $\varepsilon_{i}(x) \arrowlim{x\to a}0_{F_{i}}$, $L: \applic{E}{F_{1} \times  \cdots \times F_{q}}{h}{\big(\dd f_{i}(a)\mdot h\big)_{1 \leq  i \leq q}}$ linéaire et $\varepsilon(x) = \big(\varepsilon_{i}(x)\big)_{1 \leq  i \leq q}\arrowlim{x\to a}(0_{F_{i}})_{1 \leq  i \leq q} = 0_{F_{1} \times \cdots \times F_{q}}$. Ainsi, $f$ est différentiable en $a$ et $\dd f(a) = L$.
  2. - $f_{i} = e_{i}^{*}\circ f$ et $e_{i}^{*}$ est linéaire donc différentiable. Ainsi, le sens direct est vrai.
     - Supposons chaque $f_{i}$ différentiable en $a$.
       $$
         \begin{align*}
           \forall x \in U, f(x) &= \sum_{i=1}^{q}\big(f_{i}(a) + \dd f_{i}(a)\mdot (x-a) + \norm{x-a}\varepsilon_{i}(x)\big)e_{i}\\
           &=  f(a) + L \mdot (x-a) + \norm{x-a}\underbrace{\sum_{i=1}^{q}\varepsilon_{i}(x)e_{i}}_{\arrowlim{x\to a}0_{F}}
         \end{align*}
       $$
       avec $\varepsilon_{i}(x) \arrowlim{x\to a}0_{\R}$ et $L: \applic{E}{F_{q}}{h}{\sum_{i=1}^{q}\big(\dd f_{i}(a)\mdot h\big)e_{i}}$ linéaire. Ainsi, $f$ est différentiable en $a$ et $\dd f(a) = L$.
%

%eg
  - Soit $E$ un espace vectoriel euclidien. Considérons
    $$
      f: \applic{E^{2}}{\R \times E}{(u,v)}{(\scalar{u}{v} , u+v)}
    $$
    ~ $(u,v) \longmapsto \scalar{u}{v}$ est bilinéaire donc différentiable
    ~ $L:(u,v) \longmapsto u+v$ est linéaire donc différentiable et $\dd L(u,v) = L$
    donc $f$ est différentiable sur $E^{2}$ et
    $$
      \forall (u,v)\in E^{2}, \forall (h,k) \in E^{2}, \dd f(u,v)\mdot (h, k) = \big(\scalar{h}{v} + \scalar{u}{k} \big)
    $$
  - Soit $E$ un espace vectoriel euclidien. Considérons
    $$
      G: \applic{E^{2}}{\M_{2}(\R)}{(u,v)}{\mtx{\norm{u}^{2} & \scalar{u}{v} \\ \scalar{v}{u} & \norm{v}^{2}}}
    $$
    ~ $\pi: (u,v) \longmapsto \scalar{u}{v}$ est bilinéaire donc différentiable
    Posons
    ~ $\alpha: (u,v) \longmapsto \norm{u}^{2}$  ~ $\beta: (u,v) \longmapsto \norm{v}^{2}$  ~ $L: (u,v)\longmapsto (u,u)$  ~ $M: (u,v) \longmapsto (v,v)$
    On a alors $\alpha = \pi\circ L$ et $\beta = \pi\circ M$ donc, comme $L$ et $M$ sont linéaires, $\alpha$ et $\beta$ sont différentiables, et
    $$
      \dd \alpha(u,v) = \dd (\pi\circ L)(u,v) = \dd \pi(L(u,v)) \circ \dd L(u,v) = \dd \pi(u,u)\circ L
    $$
    donc
    $$
      \forall (h,k)\in E^{2}, \dd \alpha(u,v)\mdot (h,k) = \dd \pi(u,v)\mdot (h,k) = \scalar{h}{u} + \scalar{u}{k}  = 2 \scalar{h}{u}
    $$
    donc $G$ est différentiable sur $E^{2}$ et
    $$
      \forall (u,v) \in E^{2}, \dd G(u,v): \applic{E^{2}}{\M_{2}(\R)}{(h,k)}{\mtx{2 \scalar{h}{u} & \scalar{h}{v} + \scalar{u}{k} \\ \scalar{h}{v}  + \scalar{h}{k} & 2 \scalar{k}{v}  }}
    $$
%

%prop
  Soit $M: E_{1} \times \cdots \times E_{p} \longrightarrow F$ une application multilinéaire. Soient $f_{1}: U \longrightarrow E_{1}$, …, $f_{p}: U\longrightarrow E_{p}$ des application différentiables sur $U$ (ou seulement en $a \in U$). Alors
  $$
    M(f_{1}, \ldots f_{p}): x \longmapsto M(f_{1}(x), \ldots, f_{p}(x))
  $$
  est différentiable sur $U$ (ou seulement en $a$) et((notation abusive))
  $$
    \dd M(f_{1}, \ldots, f_{p}) = \sum_{i=1}^{p}M(f_{1}, \ldots, f_{i-1}, \dd f_{i}, f_{i+1}, \ldots, f_{p})
  $$
%

%proof
  $M(f_{1}, \ldots, f_{p})$ est différentiable en $a$ par composition : $U \overset{(f_{1}, \ldots, f_{p})}\longmapsto (f_{1}(x), \ldots, f_{p}(x)) \overset M \longmapsto F$, et
  $$
    \dd M(f_{1}, \ldots, f_{p})(a) = \dd M(f_{1}(a), \ldots, f_{p}(a)) \circ \dd f(1, \ldots, f_{p})(a)
  $$
Soit $h \in E$.
$$
  \dd M(f_{1}, \ldots, f_{p})(a)\mdot h = \dd M(f_{1}(a), \ldots, f_{p}(a))\mdot (\dd f_{1}(a)\mdot h, \ldots, \dd f_{p}(a)\mdot h) = \sum_{i=1}^{p}M(f_{1}(a), \ldots, f_{i-1}(a), \dd f_{i}(a)\mdot h, f_{i+1}(a), \ldots, f_{p}(a))
$$
%

%cor
  1. Si $F$ est une $R$-algèbre normée (de dimension finie) et si $f_{1}: U \longrightarrow F$, …, $f_{p}: U \longrightarrow F$ sont différentiables sur $eu$, alors $f_{1} \times  \cdots \times  f_{p}: U \longrightarrow F$ est différentiable sur $U$ et
     $$
       \dd (f_{1} \times  \cdots \times  f_{p}) = \sum_{i=1}^{p}f_{1} \times  \cdots \times  f_{i-1} \times  \dd f_{i} \times f_{i+1}\times  \cdots \times  f_{p}
     $$
  2. Toute application polynomiale en les coordonnées dans une base de $E$ est différentiable sur $E$.
  3. Toute fonction rationnelle en les coordonnées dans une base est différentiable sur son ouvert de définition.
%

%proof
  1. $M: \applic{F^{p}}{F}{(u_{1}, \ldots, u_{p})}{u_{1} \times  \cdots \times  u_{p}}$ est $p$-linéaire et $f_{1} \times  \cdots \times  f_{p} = M(f_{1}, \ldots, f_{p})$
  2. Soit $(e_{1}, \ldots, e_{n})$ une base de $E$. En notant $P = \sum_{(i_{1}, \ldots, i_{m}) \in I}a_{i_{1}, \ldots, i_{n}}X_{1}^{i_{1}} \times  \cdots \times  X_{n}^{i_{n}}$ avec $I$ une partie finie de $\N^{n}$.
     $$
       \tilde{P}: \applic{E}{\R}{x = \sum_{i=1}^{n}x_{i}e_{i}}{\tilde{P}(x_{1}, \ldots, x_{n})}
     $$
     donc
     $$
       \tilde{P} = \sum_{(i_{1}, \ldots, i_{n}) \in I} a_{i_{1}, \ldots, i_{n}} (e_{1}^{*})^{i_{1}} \times \cdots \times (e_{n}^{*})^{i_{n}}
     $$
     chaque $e_{i}^{*}$ est une forme linéaire donc différentiable sur $E$, donc tout produit $(e_{1}^{*})^{i_{1}} \times  \cdots \times  (e_{n}^{*})^{i_{n}}$ est différentiable sur $E$. Par combinaison linéaire, $\tilde{P}$ est différentiable sur $E$.
  3. $\frac{\tilde{P}}{\tilde{Q}} = \tilde{P} \times  \frac{1}{\tilde{Q}}$ et $\frac{1}{\tilde{Q}} = I\circ \tilde{Q}$ avec $I: \applic{\R^{*}}{\R}{t}{\frac{1}{t}}$ est différentiable.
%

%eg
  - Considérons $f: \applic{\R^{*} \times \R}{\R^{2}}{(x,y)}{\left(\frac{y}{x}, x+y\right)}$. $(x,y) \longmapsto \frac{y}{x}$ est différentiable sur $\R^{*} \times \R$ et $(x,y) \longmapsto x+y$ est différentiable sur $\R^{2}$ donc $f$ est différentable sur $\R^{*} \times \R$.
  - Le déterminant $\det : \applic{\M_{n}(\R)}{\R}{A}{\sum_{\sigma \in \S_{n}}\varepsilon(\sigma) a_{\sigma,1} \cdots a_{\sigma(n), n}}$ est différentiable sur $\M_{n}(\R)$. Soit $A \in \M_{n}(\R)$. $\dd (\det )(A): \M_{n}(\R) \longrightarrow \R$ est linéaire. Soit $H \in \M_{n}(\R)$.
    $$
      \dd (\det )(A)\mdot H = D_{H} \det (A) = \varphi'(0)
    $$
    avec $\varphi: t \longmapsto \det (A+t H)$. Alors $\varphi'(t) = \tr (com(A+tH)\transp H)$. Ainsi,
    $$
      \dd (\det )(A): \applic{\M_{n}(\R)}{\R}{H}{\tr \big((\com A)\transp H\big)}
    $$
  - Considérons $I: \applic{\GL_{n}(\R)}{\M_{n}(\R)}{A}{A^{-1} = \frac{1}{\det A}(\com A)\transp}$. $A \longmapsto (A^{-1})_{i,j}$ est une fonction rationnelle en les coordonnées dans la base canonique donc est différentiable sur l’ouvert $\GL_{n}(\R)$.
%

## Équations aux dérivées partielles d’ordre 1

%prop
  Soit $I$ un intervalle réel ouvert non vide. Soit $U \subset \R^{n}$ un ouvert non vide. Soit $f: \applic{I \times U}{F}{(x,y)}{f(x,y)}$ une application différentiable sur $I \times U$. On a
  $$
    \partial_{1}f = 0 \sur I \times U \iff \text{il existe } \lambda: U \longrightarrow  F \text{ différentiable telle que } \forall (x,y)\in I \times U, f(x,y) = \lambda(y)
  $$
%

%proof
  - Supposons $\partial_{1} f = 0$ sur $I \times U$. Soient $y \in U$, $(x,x') \in I^{2}$ et $\gamma: \applic{\cc{0,1}}{F}{t}{\underbrace{f(x+t(x'-x), y)}_{\in I \times U}}$. $\gamma$ est alors dérivable et
    $$
      \forall t \in \cc{0,1}, \gamma'(t) = (x'-x)\partial_{1}f(x+t(x'-x), y) + \sum_{i=2}^{n+1}0 \times \partial_{i}f(x+t(x'-x),y)
    $$
    d’où $\forall t \in \cc{0,1}, \gamma'(t) = 0_{F}$, donc $\gamma(1) = \gamma(0)$, i.e.$f'(x',y) = f(x,y)$. Sinsi, pour $x_{0}\in I$ fixé, on a
    $$
      \forall (x,y) \in I \times U, f(x,y) = \underbrace{f(x_{0},y)}_{\lambda(y)}
    $$
    et $\lambda: \applic{U}{F}{y}{f(x_{0},y)}$ est différentiable comme composée des fonctions $y \longmapsto (x_{0},y)$ et $f:(x_{0},y) \longmapsto f(x_{0},y)$.
  - La réciproque est immédiate.
%

%eg
  Trouvons $f: (x,y,z) \longmapsto f(x,y,z) \in \R$ différentiable telle que
  $$
    \tag{$\star$} \partial_{1}f(x,y,z) = \frac{x}{\ln(x+z)}
  $$
  $f$ est définie sur $\R \times H$ avec $H = \{(y,z) \in \R^{2} \where y+z > 0 \and y+z \ne 1 \}$. $H$ est un ouvert de $\R^{2}$.
  $$
    \begin{align*}
      (\star) &\iff  \pdv{}{x}\left(f(x,y,z) - \frac{x^{2}}{2\ln (y+z)}\right) = 0 \sur \R \times H\\
              &\iff \text{il existe } \lambda: H \longrightarrow \R \text{ différentiable telle que } \forall (x,y,z) \in \R, f(x,y,z) = \frac{x^{2}}{2\le(y+z)} + \lambda(y,z)
    \end{align*}
  $$
%

%eg
  Trouvons $f: \R \times H \longrightarrow \R$ différentiable telle que
  $$
    \tag{$\triangle$} \partial_{1} f = \frac{x}{\ln(y+z)}f
  $$
  $$
    \begin{align*}
      (\triangle) &\iff \partial_{1} f \times e^{\frac{x^{2}}{2\ln (y+z)}} - \frac{x}{\ln(y+z)} e^{\frac{x^{2}}{e^{2\ln(y+z)}}}f = 0\\
&\iff \pdv{}{x}\big(f e^{-\frac{x^{2}}{2\ln(y+z)}}\big) = 0\\
&\iff \exists \lambda: H \longrightarrow \R \text{ différentiable telle que } \forall (x,y,z) \in \R \times H, f(x,y,z) e^{-\frac{x^{2}}{2\ln(y+z)}} = \lambda(y,z)
    \end{align*}
  $$
  Les solutions sont donc
  $$
    \Sol_{\R \times H} = \left\{(x,y,z) \longmapsto \lambda(y,z) e^{\frac{x^{2}}{2\ln(y+z)}}\where \lambda: H \longrightarrow \R \text{ différentiable}\right\}
  $$
%

%prop Chain rule
  Soit
  $$
    g: \applic{V \subset \R^{p}}{F}{(v_{1}, \ldots, b_{p})}{f(x_{1}(v_{1}, \ldots, v_{p}), \ldots, x_{n}(v_{1}, \ldots, v_{p}))}
  $$
  où $V$ est un ouvert de $\R^{p}$.
  $$
    f: \applic{U \subset \R^{n}}{F}{(x_{1}, \ldots, x_{n})}{f(x_{1}, \ldots, x_{n})}
  $$
  est différentiable en sur $U$ et $(v_{1}, \ldots, v_{n}) \longmapsto (x_{1}(v_{1}, \ldots, v_{n}), \ldots, x_{n}(v_{1}, \ldots, v_{p}))$ est différentiable sur $V$ à valeurs dans $U$. Alors $g$ est différentiable sur $V$ et
  $$
    V(v_{1}, \ldots, v_{p}) \in V^{p}, \pdv{g}{v_{i}} (v_{1}, \ldots, v_{p}) = \sum_{k=1}^{n} \pdv{x_{k}}{v_{i}}(v_{1}, \ldots, v_{p}) \times \underbrace{\pdv{f}{x_{k}}(x_{1}(v_{1}, \ldots, v_{p}), \ldots, x_{n}(v_{1}, \ldots, v_{p}))}_{\in F}
  $$
  aussi écrit
  $$
    \partial_{i} g(v_{1}, \ldots, v_{p}) = \sum_{k=1}^{n}\partial_{i}x_{k}(v_{1}, \ldots, v_{p}) \times \partial_{k}f(x_{1}(v_{1}, \ldots, v_{p}), \ldots, x_{n}(v_{1}, \ldots, v_{p}))
  $$
  Si $f$ est à valeurs réelles, alors on écrit
  $$
    \pdv{g}{v_{1}}(v) = \sum_{k=1}^{n}\pdv{f}{x_{k}}(x_{1}(v), \ldots, x_{n}(v)) \times \pdv{x_{k}}{v_{i}}(v)
  $$
  ou par abus,
  $$
    \pdv{g}{v_{i}} = \sum_{i=1}^{n}\pdv{f}{x_{k}} \pdv{x_{k}}{v_{i}}
  $$
%

%proof
  - $g = f\circ (x_{1}, \ldots, x_{p})$ est différentiable sur $V$.
  - Soit $(v_{1}, \ldots, v_{p}) \in V$. $t \longmapsto  f(x_{1}(v_{1}, \ldots, t, \ldots, v_{p}), \ldots, x_{n}(v_{1}, \ldots, t, \ldots, v_{p}))$ est dérivable en $v_{i}$ (dérivée le long d’un arc) et de dérivée en $v_{i}$ valant
    $$
      \sum_{k=1}^{n}\eval{\dv{}{t} \big(x_{k}(v_{1}, \ldots, t, \ldots, v_{p})\big)}{t=v_{i}} \pdv{f}{x_{k}}(x(v)) = \sum_{k=1}^{n}\pdv{x_{k}}{v_{i}}(v) \times \pdv{f}{x_{k}}(x(v))
    $$
%

%eg
  Déterminons les applications différentiables $f: \R^{3} \longrightarrow \R$ telles que
  $$
    \tag{$\oslash$} \partial_{1} f + \partial_{2} f + \partial_{3} f = 0
  $$
  Pour cela, posons
  ~ $u = x-y$  ~ $v = y-z$  ~ $w = z$
  Soit l’application $\varphi: \applic{\R^{3}}{\R}{(x,y,z)}{(x-y, y-z, z)} \in \L(\R^{3})$. Alors
  $$
    \det \varphi = \vmtx{1&-1&0\\0&1&-1\\0&0&1} \ne 0
  $$
  donc $\varphi \in \GL(\R^{3})$. Ainsi, $\varphi$ et $\varphi^{-1}$ sont différentiables. On pose $«\tilde{f}(u,v,w) = f(x,y,z)»$, i.e. $f(x,y,z) = \tilde{f(x-y, y-z, z)}$, i.e. $f = \tilde{f} \circ \varphi$ ou encore $\tilde{f} = f\circ \varphi^{-1}$. Comme $\varphi$ est un *difféomorphisme*, on a
  $$
    f \text{ différentiable sur } \R^{3} \iff \tilde{f} \text{ différentiable sur } \R^{3}
  $$
  $$
    \pdv{f}{x} = \pdv{\tilde{f}}{u} \pdv{u}{x} + \pdv{\tilde{f}}{v} \pdv{v}{x} + \pdv{\tilde{f}}{w}\pdv{w}{x}
  $$
  i.e.
  ~ $\pdv{f}{x} = \pdv{\tilde{f}}{u}$ ou de façon correcte $\pdv{f}{x}(x,y,z) = \pdv{\tilde{f}}{u}(x-y, y-z,z)$  ~ $\pdv{f}{y} = - \pdv{\tilde{f}}{u} + \pdv{\tilde{f}}{v}$  ~ $\pdv{f}{z} = - \pdv{f}{v} + \pdv{\tilde{f}}{w}$
  Alors,
  $$
    \begin{align*}
      (\oslash) &\iff \pdv{\tilde{f}}{w} = 0\\
      &\iff \exists \lambda: \R^{2} \longrightarrow \R \text{ différentiable telle que } \forall (u,v,w) \in \R^{3}, \tilde{f}(u,v,w) = \lambda(u,v)\\
      &\iff \exists \lambda:\R^{2} \longrightarrow \R \text{ différentiable telle que } \forall (x,y,z) \in \R^{3}, f(x,y,z) = \lambda(x-y, y-z)
    \end{align*}
  $$
  Ainsi,
  $$
    \Sol = \Big\{(x,y,z) \in \R^{3} \longmapsto \lambda(x,y,y-z) \where \lambda: \R^{2} \longrightarrow \R \text{ différentiable}\Big\}
  $$
%

%eg
  Trouvons les fonctions différentiables $f:\R_{+}^{*} \times \R \longrightarrow \R$ telles que, pour $\alpha \in \R$,
  $$
    \tag{$\ltimes$} x \pdv{f}{x} + y \pdv{f}{y} = \alpha f
  $$
  Pour cela, posons
  ~ $x = r\cos \theta$  ~ $y = r\sin \theta$
et
$$
  \varphi: \applic{\R_{+}^{*} \times \oo{\frac{\pi}{2}, \frac{\pi}{2}}}{\R_{+}^{*} \times \R}{r,\theta}{\left(\underbrace{r\cos \theta}_{x(r,\theta)}, \underbrace{r\sin \theta}_{y(r, \theta)}\right)}
$$
On a
$$
  \varphi^{-1}: \applic{\R_{+}^{*} \times \R}{\R_{+}^{*} \times \oo{- \frac{\pi}{2}, \frac{\pi}{2}}}{(x,y)}{\left(\sqrt{x^{2}+y^{2}}, \arctan \left(\frac{y}{x}\right)\right)}
$$
donc $\varphi$ et $\varphi^{-1}$ différentiables. On pose $\tilde{f}(r, \theta) = f(x,y)$ i.e. $\tilde{f}(r, \theta) = f(r\cos \theta, r\sin \theta)$, de sorte que $\tilde{f = f\circ \varphi}$, i.e $f = \tilde{f \circ \varphi^{-1}}$. Comme $\varphi$ est un *difféomorphisme*,
$$
  f \text{ différentiable sur } \R_{+}^{*} \times \R \iff \tilde{f} \text{ différentiable sur } \R_{+}^{*} \times \oo{- \frac{\pi}{2}, \frac{\pi}{2}}
$$
$$
  \pdv{\tilde{f}}{r} = \pdv{f}{x} \pdv{x}{r} + \frac{f}{y} \frac{y}{r}
$$
$$
  \pdv{\tilde{f}}{r} = \cos \theta \pdv{f}{x} + \sin \theta \pdv{f}{y}
$$
$$
  r \pdv{\tilde{f}}{r} = x \pdv{f}{y} + y \pdv{f}{y}
$$
$$
  \begin{align*}
    (\ltimes) &\iff r \pdv{\tilde{f}}{r} = \alpha f\\
&\iff \pdv{\tilde{f}}{r} = \frac{\alpha}{r}f \\
&\iff \exists \lambda \in \Diff^{1}\left(\oo{-\frac{\pi}{2}, \frac{\pi}{2}}\right), \forall (r, \theta) \in \R_{+}^{*} \times \oo{-\frac{\pi}{2}, \frac{\pi}{2}}, \tilde{f}(r, \theta) = \lambda(\theta) e^{\alpha\ln r} = \lambda(\theta)r^\alpha
  \end{align*}
$$
Ainsi, les solutions sont
$$
  \Sol = \left\{(x,y) \in \R_{+}^{*} \times \R \longmapsto \lambda \left(\arctan \left(\frac{y}{x}\right)\right) (x^{2}+y^{2})^{\frac{\alpha}{2}}\where \lambda \in \Diff^{1}\left(\oo{- \frac{\pi}{2}, \frac{\pi}{2}}\right)\right\}
$$
soit
$$
  \Sol = \left\{(x,y) \longmapsto \mu \left(\frac{y}{x}(x^{2}+y^{2})^{\frac{\alpha}{2}}\right) \where \mu \in \Diff^{1}(\R)\right\}
$$

%
